{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ca1c95-f4aa-4601-8586-9fc319260a89",
   "metadata": {},
   "source": [
    "## RL Hacking 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889b727-6c1a-405b-b57f-41040aac25eb",
   "metadata": {},
   "source": [
    "Hack in this direction as long as it makes sense: \n",
    "\n",
    "- [ ]  Setup a basic LLM (maybe QWEN 0.6B)\n",
    "- [ ]  A math dataset\n",
    "- [ ]  Run various policy gradient methods to teach the model to reason (GRPO, Raschka’s simpler GRPO, Dr. GRPO, PPO, Vanilla Policy gradient, maybe DPO, maybe RPOO)\n",
    "- [ ]  How do implementations perform and vary? Do components make sense? Can we replicated some version of the “aha moment”? (although that’s apparently not an emergent RL property?) Does the way the math is setup make sense? does Karpathy’s simplified explanation make it easier? Would demoing on or more of these algorithms on in a game setting make more sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ecc5ae-8cdb-4f1d-a6c2-58ea3bc2e26e",
   "metadata": {},
   "source": [
    "I'd prefer to use standard HF QWEN here, but let's roll with Raschka's rooling for a bit\n",
    "- https://github.com/rasbt/reasoning-from-scratch/tree/main/ch06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3eb425-16fc-4f03-a983-d8185fc5cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA GPU\n",
      "✓ qwen3/qwen3-0.6B-base.pth already up-to-date\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from reasoning_from_scratch.ch02 import get_device\n",
    "from reasoning_from_scratch.ch03 import (\n",
    "     load_model_and_tokenizer\n",
    ")\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    which_model=\"base\",\n",
    "    device=device,\n",
    "    use_compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fefadd2-2b5b-43de-9647-c6b85216f042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46"
     ]
    }
   ],
   "source": [
    "from reasoning_from_scratch.ch03 import render_prompt\n",
    "from reasoning_from_scratch.ch04 import (\n",
    "    generate_text_stream_concat_flex,\n",
    "    generate_text_top_p_stream_cache\n",
    ")\n",
    "\n",
    "raw_prompt = (\n",
    "    \"Half the value of $3x-9$ is $x+37$. \"\n",
    "    \"What is the value of $x$?\"\n",
    ")\n",
    "prompt = render_prompt(raw_prompt)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "response = generate_text_stream_concat_flex(\n",
    "    model, tokenizer, prompt, device,\n",
    "    max_new_tokens=2048, verbose=True,\n",
    "    generate_func=generate_text_top_p_stream_cache,\n",
    "    temperature=0.9,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222dd76e-cf07-465a-a10a-a7a4667d2168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 46'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b86c22-a498-4642-ac7d-bbc14de59181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def load_math_train(local_path=\"math_train.json\", save_copy=True):\n",
    "    local_path = Path(local_path)\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"math_full_minus_math500/refs/heads/main/\"\n",
    "        \"math_full_minus_math500.json\"\n",
    "    )\n",
    "\n",
    "    if local_path.exists():\n",
    "        with local_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        if save_copy:  # Saves a local copy\n",
    "            with local_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df4e282b-cdab-4670-9f0c-335a0a0079c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 12000\n"
     ]
    }
   ],
   "source": [
    "math_train = load_math_train()\n",
    "\n",
    "print(\"Dataset size:\", len(math_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e3b145d-28ea-4654-a98a-4afb07a98f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '6',\n",
      " 'level': 'Level 3',\n",
      " 'problem': 'Sam is hired for a 20-day period. On days that he works, he earns '\n",
      "            '$\\\\$$60. For each day that he does not work, $\\\\$$30 is '\n",
      "            'subtracted from his earnings. At the end of the 20-day period, he '\n",
      "            'received $\\\\$$660. How many days did he not work?',\n",
      " 'solution': 'Call $x$ the number of days Sam works and $y$ the number of days '\n",
      "             'he does not. We can set up the following system of equations to '\n",
      "             'represent the given information: \\\\begin{align*}\\n'\n",
      "             'x+y &= 20 \\\\\\\\\\n'\n",
      "             '60x - 30y &= 660 \\\\\\\\\\n'\n",
      "             '\\\\end{align*} The first equation represents the total number of '\n",
      "             'days Sam works, and the second equation represents his total '\n",
      "             'profit. Solving for $x$ in the first equation yields $x = 20 - '\n",
      "             'y$. Substituting into the second equation gives $60(20-y) - 30y '\n",
      "             '= 660$. Canceling a factor of $10$ and multiplying out gives '\n",
      "             '$120 - 6y - 3y = 66$. This simplifies to $-9y = -54$, or $y = '\n",
      "             '6$. Thus, Sam did not work for $\\\\boxed{6}$ days.',\n",
      " 'type': 'Algebra',\n",
      " 'unique_id': 4}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(math_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe69bd99-8f4c-4877-83ce-60dbcac13583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reasoning_from_scratch.qwen3 import KVCache\n",
    "from reasoning_from_scratch.ch04 import top_p_filter\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_response(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    device,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    input_ids = torch.tensor(\n",
    "        tokenizer.encode(prompt),\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "    model.reset_kv_cache()\n",
    "    logits = model(input_ids.unsqueeze(0), cache=cache)[:, -1]\n",
    "\n",
    "    generated = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        if temperature and temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        probas = top_p_filter(probas, top_p)\n",
    "        next_token = torch.multinomial(\n",
    "            probas.cpu(), num_samples=1\n",
    "        ).to(device)\n",
    "\n",
    "        if (\n",
    "            tokenizer.eos_token_id is not None\n",
    "            and next_token.item() == tokenizer.eos_token_id\n",
    "        ):\n",
    "            break\n",
    "        generated.append(next_token.item())\n",
    "        logits = model(next_token, cache=cache)[:, -1]\n",
    "\n",
    "    full_token_ids = torch.cat(\n",
    "        [input_ids,\n",
    "         torch.tensor(generated, device=device, dtype=input_ids.dtype),]\n",
    "    )\n",
    "    return full_token_ids, input_ids.numel(), tokenizer.decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7af530-e5dc-4c19-9821-7007a74629a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "raw_prompt = (\n",
    "    \"Half the value of $3x-9$ is $x+37$. \"\n",
    "    \"What is the value of $x$?\"\n",
    ")\n",
    "prompt = render_prompt(raw_prompt)\n",
    "\n",
    "token_ids, prompt_len, answer_text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "print(answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511bd9ad-197f-4866-9504-e6561642c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let's solve the problem step by step.\n",
      "\n",
      "1. **Translate the problem into an equation:**\n",
      "   \n",
      "   The problem states that half the value of \\( 3x - 9 \\) is \\( x + 37 \\). We can translate this into the following equation:\n",
      "   \\[\n",
      "   \\frac{1}{2}(3x - 9) = x + 37\n",
      "   \\]\n",
      "\n",
      "2. **Simplify the equation:**\n",
      "   \n",
      "   Distribute the \\( \\frac{1}{2} \\) on the left side:\n",
      "   \\[\n",
      "   \\frac{3x}{2} - \\frac{9}{2} = x + 37\n",
      "   \\]\n",
      "\n",
      "3. **Eliminate the fraction by multiplying every term by 2:**\n",
      "   \\[\n",
      "   3x - 9 = 2x + 74\n",
      "   \\]\n",
      "\n",
      "4. **Isolate the variable \\( x \\):**\n",
      "   \n",
      "   Subtract \\( 2x \\) from both sides:\n",
      "   \\[\n",
      "   3x - 2x - 9 = 74\n",
      "   \\]\n",
      "   \\[\n",
      "   x - 9 = 74\n",
      "   \\]\n",
      "\n",
      "5. **Solve for \\( x \\):**\n",
      "   \n",
      "   Add 9 to both sides:\n",
      "   \\[\n",
      "   x = 74 + 9\n",
      "   \\]\n",
      "   \\[\n",
      "   x = 83\n",
      "   \\]\n",
      "\n",
      "6. **Write the final answer:**\n",
      "   \n",
      "   \\[\n",
      "   \\boxed{83}\n",
      "   \\]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "token_ids, prompt_len, answer_text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "print(answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb90b569-c6f5-4b76-8d2e-deda9c6324c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let's solve the problem step by step.\n",
      "\n",
      "**Given:**\n",
      "\\[\n",
      "\\frac{1}{2} \\times (3x - 9) = x + 37\n",
      "\\]\n",
      "\n",
      "**Step 1: Eliminate the fraction by multiplying both sides by 2.**\n",
      "\\[\n",
      "2 \\times \\left( \\frac{1}{2} \\times (3x - 9) \\right) = 2 \\times (x + 37)\n",
      "\\]\n",
      "\\[\n",
      "3x - 9 = 2x + 74\n",
      "\\]\n",
      "\n",
      "**Step 2: Subtract \\(2x\\) from both sides to get the \\(x\\)-terms on one side.**\n",
      "\\[\n",
      "3x - 2x - 9 = 74\n",
      "\\]\n",
      "\\[\n",
      "x - 9 = 74\n",
      "\\]\n",
      "\n",
      "**Step 3: Add 9 to both sides to solve for \\(x\\).**\n",
      "\\[\n",
      "x = 74 + 9\n",
      "\\]\n",
      "\\[\n",
      "x = 83\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\\[\n",
      "\\boxed{83}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "\n",
    "token_ids, prompt_len, answer_text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "print(answer_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ba6c5-aad6-485a-aab1-6e945632e3bd",
   "metadata": {},
   "source": [
    "- It is cool that sometimes QWEN kidna reasons here and sometimes it doesn't.\n",
    "- Wonder how this various across models\n",
    "- Ok so from here Raschka uses some simulated rollouts to walk through GPRO math, that's cool.\n",
    "- The general premise of how do we train our model to get the right answer more often is nice\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4996fb-d197-4937-876e-e31abb139afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658f1ed-4d80-47df-800f-e8fdc1e2663a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b90d1f-273e-44e4-9b2b-cd025f1a634e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee0c60-bfea-4670-a461-4479752475d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83749b45-d7fd-4cdb-83b9-529bff7ed36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5baf6f-3087-4a1f-85e2-ef4be54942d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d0569-5854-4e48-9a0b-79b11fc585dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cc0c4-9f92-4099-b37f-1f1deace7742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8264ab-c40b-488b-a6cb-f8ec1141cda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca1882a-6d1a-41d3-96a2-2c2dd7f62eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a099aad4-d151-4a35-be91-7722c4736b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "# MODEL_NAME = \"Qwen/qwen3-0.6B-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6860cfa-17e8-4120-8dcb-28df767d247f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1923c6aa62e940299fb83449423b492c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac4c25f640c4e8fbe0fc246c14b6706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04510c25b47c4f8e95bf009933043670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=0,\n",
    "# )\n",
    "# # reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "# #     MODEL_NAME,\n",
    "# #     attn_implementation=\"flash_attention_2\",\n",
    "# #     torch_dtype=torch.bfloat16,\n",
    "# #     device_map=0,\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aff8f3-e790-41c3-bb63-956557b69035",
   "metadata": {},
   "source": [
    "- Hmm yeah this is like a non-trivial amount of tooling with either repo\n",
    "- Raschka's seems friendlier, but more complex than I would want in some areas for sure.\n",
    "- Can I achieve my hacking goals here using Raschka's code?\n",
    "- Seems like it comes down to if I can make sense of the policy gradient part of his code and modify it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb7869-c152-48f9-9104-68561805fd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02071b95-008c-418e-8559-f815bd7d7b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfab81-86af-460c-a589-b47d05836223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48a1e9-0c05-4272-b724-92916a86afc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

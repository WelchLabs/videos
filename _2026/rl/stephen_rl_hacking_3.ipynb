{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ca1c95-f4aa-4601-8586-9fc319260a89",
   "metadata": {},
   "source": [
    "## RL Hacking 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889b727-6c1a-405b-b57f-41040aac25eb",
   "metadata": {},
   "source": [
    "Hack in this direction as long as it makes sense: \n",
    "\n",
    "- [ ]  Setup a basic LLM (maybe QWEN 0.6B)\n",
    "- [ ]  A math dataset\n",
    "- [ ]  Run various policy gradient methods to teach the model to reason (GRPO, Raschka’s simpler GRPO, Dr. GRPO, PPO, Vanilla Policy gradient, maybe DPO, maybe RPOO)\n",
    "- [ ]  How do implementations perform and vary? Do components make sense? Can we replicated some version of the “aha moment”? (although that’s apparently not an emergent RL property?) Does the way the math is setup make sense? does Karpathy’s simplified explanation make it easier? Would demoing on or more of these algorithms on in a game setting make more sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ecc5ae-8cdb-4f1d-a6c2-58ea3bc2e26e",
   "metadata": {},
   "source": [
    "I'd prefer to use standard HF QWEN here, but let's roll with Raschka's rooling for a bit\n",
    "- https://github.com/rasbt/reasoning-from-scratch/tree/main/ch06\n",
    "- https://github.com/McGill-NLP/nano-aha-moment/blob/main/nano_r1.ipynb\n",
    "- https://github.com/rasbt/reasoning-from-scratch/blob/main/ch07/01_main-chapter-code/ch07_main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3eb425-16fc-4f03-a983-d8185fc5cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA GPU\n",
      "✓ qwen3/qwen3-0.6B-base.pth already up-to-date\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from reasoning_from_scratch.ch02 import get_device\n",
    "from reasoning_from_scratch.ch03 import (\n",
    "     load_model_and_tokenizer\n",
    ")\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    which_model=\"base\",\n",
    "    device=device,\n",
    "    use_compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fefadd2-2b5b-43de-9647-c6b85216f042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46"
     ]
    }
   ],
   "source": [
    "from reasoning_from_scratch.ch03 import render_prompt\n",
    "from reasoning_from_scratch.ch04 import (\n",
    "    generate_text_stream_concat_flex,\n",
    "    generate_text_top_p_stream_cache\n",
    ")\n",
    "\n",
    "raw_prompt = (\n",
    "    \"Half the value of $3x-9$ is $x+37$. \"\n",
    "    \"What is the value of $x$?\"\n",
    ")\n",
    "prompt = render_prompt(raw_prompt)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "response = generate_text_stream_concat_flex(\n",
    "    model, tokenizer, prompt, device,\n",
    "    max_new_tokens=2048, verbose=True,\n",
    "    generate_func=generate_text_top_p_stream_cache,\n",
    "    temperature=0.9,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222dd76e-cf07-465a-a10a-a7a4667d2168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 46'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b86c22-a498-4642-ac7d-bbc14de59181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def load_math_train(local_path=\"math_train.json\", save_copy=True):\n",
    "    local_path = Path(local_path)\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"math_full_minus_math500/refs/heads/main/\"\n",
    "        \"math_full_minus_math500.json\"\n",
    "    )\n",
    "\n",
    "    if local_path.exists():\n",
    "        with local_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        if save_copy:  # Saves a local copy\n",
    "            with local_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4e282b-cdab-4670-9f0c-335a0a0079c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 12000\n"
     ]
    }
   ],
   "source": [
    "math_train = load_math_train()\n",
    "\n",
    "print(\"Dataset size:\", len(math_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3b145d-28ea-4654-a98a-4afb07a98f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '6',\n",
      " 'level': 'Level 3',\n",
      " 'problem': 'Sam is hired for a 20-day period. On days that he works, he earns '\n",
      "            '$\\\\$$60. For each day that he does not work, $\\\\$$30 is '\n",
      "            'subtracted from his earnings. At the end of the 20-day period, he '\n",
      "            'received $\\\\$$660. How many days did he not work?',\n",
      " 'solution': 'Call $x$ the number of days Sam works and $y$ the number of days '\n",
      "             'he does not. We can set up the following system of equations to '\n",
      "             'represent the given information: \\\\begin{align*}\\n'\n",
      "             'x+y &= 20 \\\\\\\\\\n'\n",
      "             '60x - 30y &= 660 \\\\\\\\\\n'\n",
      "             '\\\\end{align*} The first equation represents the total number of '\n",
      "             'days Sam works, and the second equation represents his total '\n",
      "             'profit. Solving for $x$ in the first equation yields $x = 20 - '\n",
      "             'y$. Substituting into the second equation gives $60(20-y) - 30y '\n",
      "             '= 660$. Canceling a factor of $10$ and multiplying out gives '\n",
      "             '$120 - 6y - 3y = 66$. This simplifies to $-9y = -54$, or $y = '\n",
      "             '6$. Thus, Sam did not work for $\\\\boxed{6}$ days.',\n",
      " 'type': 'Algebra',\n",
      " 'unique_id': 4}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(math_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe69bd99-8f4c-4877-83ce-60dbcac13583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reasoning_from_scratch.qwen3 import KVCache\n",
    "from reasoning_from_scratch.ch04 import top_p_filter\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_response(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    device,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    input_ids = torch.tensor(\n",
    "        tokenizer.encode(prompt),\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "    model.reset_kv_cache()\n",
    "    logits = model(input_ids.unsqueeze(0), cache=cache)[:, -1]\n",
    "\n",
    "    generated = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        if temperature and temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        probas = top_p_filter(probas, top_p)\n",
    "        next_token = torch.multinomial(\n",
    "            probas.cpu(), num_samples=1\n",
    "        ).to(device)\n",
    "\n",
    "        if (\n",
    "            tokenizer.eos_token_id is not None\n",
    "            and next_token.item() == tokenizer.eos_token_id\n",
    "        ):\n",
    "            break\n",
    "        generated.append(next_token.item())\n",
    "        logits = model(next_token, cache=cache)[:, -1]\n",
    "\n",
    "    full_token_ids = torch.cat(\n",
    "        [input_ids,\n",
    "         torch.tensor(generated, device=device, dtype=input_ids.dtype),]\n",
    "    )\n",
    "    return full_token_ids, input_ids.numel(), tokenizer.decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7af530-e5dc-4c19-9821-7007a74629a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "raw_prompt = (\n",
    "    \"Half the value of $3x-9$ is $x+37$. \"\n",
    "    \"What is the value of $x$?\"\n",
    ")\n",
    "prompt = render_prompt(raw_prompt)\n",
    "\n",
    "token_ids, prompt_len, answer_text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "print(answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511bd9ad-197f-4866-9504-e6561642c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let's solve the problem step by step.\n",
      "\n",
      "1. **Translate the problem into an equation:**\n",
      "   \n",
      "   The problem states that half the value of \\( 3x - 9 \\) is \\( x + 37 \\). We can translate this into the following equation:\n",
      "   \\[\n",
      "   \\frac{1}{2}(3x - 9) = x + 37\n",
      "   \\]\n",
      "\n",
      "2. **Simplify the equation:**\n",
      "   \n",
      "   Distribute the \\( \\frac{1}{2} \\) on the left side:\n",
      "   \\[\n",
      "   \\frac{3x}{2} - \\frac{9}{2} = x + 37\n",
      "   \\]\n",
      "\n",
      "3. **Eliminate the fraction by multiplying every term by 2:**\n",
      "   \\[\n",
      "   3x - 9 = 2x + 74\n",
      "   \\]\n",
      "\n",
      "4. **Isolate the variable \\( x \\):**\n",
      "   \n",
      "   Subtract \\( 2x \\) from both sides:\n",
      "   \\[\n",
      "   3x - 2x - 9 = 74\n",
      "   \\]\n",
      "   \\[\n",
      "   x - 9 = 74\n",
      "   \\]\n",
      "\n",
      "5. **Solve for \\( x \\):**\n",
      "   \n",
      "   Add 9 to both sides:\n",
      "   \\[\n",
      "   x = 74 + 9\n",
      "   \\]\n",
      "   \\[\n",
      "   x = 83\n",
      "   \\]\n",
      "\n",
      "6. **Write the final answer:**\n",
      "   \n",
      "   \\[\n",
      "   \\boxed{83}\n",
      "   \\]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "token_ids, prompt_len, answer_text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "print(answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb90b569-c6f5-4b76-8d2e-deda9c6324c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let's solve the problem step by step.\n",
      "\n",
      "**Given:**\n",
      "\\[\n",
      "\\frac{1}{2} \\times (3x - 9) = x + 37\n",
      "\\]\n",
      "\n",
      "**Step 1: Eliminate the fraction by multiplying both sides by 2.**\n",
      "\\[\n",
      "2 \\times \\left( \\frac{1}{2} \\times (3x - 9) \\right) = 2 \\times (x + 37)\n",
      "\\]\n",
      "\\[\n",
      "3x - 9 = 2x + 74\n",
      "\\]\n",
      "\n",
      "**Step 2: Subtract \\(2x\\) from both sides to get the \\(x\\)-terms on one side.**\n",
      "\\[\n",
      "3x - 2x - 9 = 74\n",
      "\\]\n",
      "\\[\n",
      "x - 9 = 74\n",
      "\\]\n",
      "\n",
      "**Step 3: Add 9 to both sides to solve for \\(x\\).**\n",
      "\\[\n",
      "x = 74 + 9\n",
      "\\]\n",
      "\\[\n",
      "x = 83\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\\[\n",
      "\\boxed{83}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "\n",
    "token_ids, prompt_len, answer_text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "print(answer_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ba6c5-aad6-485a-aab1-6e945632e3bd",
   "metadata": {},
   "source": [
    "- It is cool that sometimes QWEN kidna reasons here and sometimes it doesn't.\n",
    "- Wonder how this various across models\n",
    "- Ok so from here Raschka uses some simulated rollouts to walk through GPRO math, that's cool.\n",
    "- The general premise of how do we train our model to get the right answer more often is nice\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4996fb-d197-4937-876e-e31abb139afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reasoning_from_scratch.ch03 import (\n",
    "    extract_final_candidate, grade_answer\n",
    ")\n",
    "\n",
    "def reward_rlvr(answer_text, ground_truth):\n",
    "    extracted = extract_final_candidate(\n",
    "        answer_text, fallback=None  # Require \\boxed{}\n",
    "    )\n",
    "    if not extracted:\n",
    "        return 0.0\n",
    "    correct = grade_answer(extracted, ground_truth)\n",
    "    return float(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f658f1ed-4d80-47df-800f-e8fdc1e2663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def avg_logprob_answer(model, tokenizer, prompt, answer, device=\"cpu\"):\n",
    "\n",
    "    # Encode prompt and answer tokens separately to get the prompt length later\n",
    "    prompt_ids = tokenizer.encode(prompt)\n",
    "    answer_ids = tokenizer.encode(answer)\n",
    "    full_ids = torch.tensor(prompt_ids + answer_ids, device=device)\n",
    "\n",
    "    # Same as in calc_next_token_logprobas before\n",
    "    logits = model(full_ids.unsqueeze(0)).squeeze(0)\n",
    "    logprobs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Index range for positions corresponding to answer tokens\n",
    "    start = len(prompt_ids) - 1\n",
    "    end = full_ids.shape[0] - 1\n",
    "\n",
    "    # Same as before, except for using start and end\n",
    "    t_idx = torch.arange(start, end, device=device)\n",
    "    next_tokens = full_ids[start + 1 : end + 1]\n",
    "    next_token_logps = logprobs[t_idx, next_tokens]\n",
    "\n",
    "    # Average over the answer token scores\n",
    "    return torch.mean(next_token_logps).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b90d1f-273e-44e4-9b2b-cd025f1a634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.802001953125\n"
     ]
    }
   ],
   "source": [
    "avg_logprob_val = avg_logprob_answer(\n",
    "                   model, tokenizer, \n",
    "                   prompt=prompt,\n",
    "                   answer=answer_text,\n",
    "                   device=device) \n",
    "\n",
    "sequence_logprob_val = avg_logprob_val * (len(tokenizer.encode(answer_text)))\n",
    "print(sequence_logprob_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3ee0c60-bfea-4670-a461-4479752475d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-12.7940, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#SW - maybe work wiht this more robust version from Raschka? Renaming here\n",
    "\n",
    "def sequence_logprob(model, token_ids, prompt_len):\n",
    "    logits = model(token_ids.unsqueeze(0)).squeeze(0).float()\n",
    "    logprobs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Positions whose next-token probabilities we want\n",
    "    # These correspond to predicting token_ids[t + 1] from position t\n",
    "    start = prompt_len - 1\n",
    "    end = token_ids.shape[0] - 1\n",
    "\n",
    "    t_idx = torch.arange(start, end, device=token_ids.device)\n",
    "    next_tokens = token_ids[start + 1 : end + 1]\n",
    "    next_token_logps = logprobs[t_idx, next_tokens]\n",
    "\n",
    "    # Sum log-probabilities over the answer tokens\n",
    "    return torch.sum(next_token_logps)\n",
    "\n",
    "print(sequence_logprob(model, token_ids, prompt_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83749b45-d7fd-4cdb-83b9-529bff7ed36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_loss(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    example,\n",
    "    device,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    assert num_rollouts >= 2\n",
    "    roll_logps, roll_rewards, samples = [], [], []\n",
    "    prompt = render_prompt(example[\"problem\"])\n",
    "\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    for _ in range(num_rollouts):\n",
    "        # Stage 1: generate rollouts\n",
    "        token_ids, prompt_len, text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        # Stage 2: compute rewards\n",
    "        reward = reward_rlvr(text, example[\"answer\"])\n",
    "        \n",
    "        # Stage 4: compute logprobs\n",
    "        logp = sequence_logprob(model, token_ids, prompt_len)\n",
    "\n",
    "        roll_logps.append(logp)\n",
    "        roll_rewards.append(reward)\n",
    "        samples.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"reward\": reward,\n",
    "                \"gen_len\": token_ids.numel() - prompt_len,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    # Stage 2: collect all rewards\n",
    "    rewards = torch.tensor(roll_rewards, device=device)\n",
    "\n",
    "    # Stage 3: compute advantages\n",
    "    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4)\n",
    "\n",
    "    # Stage 4: collect all logprobs\n",
    "    logps = torch.stack(roll_logps)\n",
    "\n",
    "    # Stage 5: compute policy gradient loss\n",
    "    pg_loss = -(advantages.detach() * logps).mean()\n",
    "    loss = pg_loss  # In the next chapter we add a KL term here\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss.item(),\n",
    "        \"pg_loss\": pg_loss.item(),\n",
    "        \"rewards\": roll_rewards,\n",
    "        \"advantages\": advantages.detach().cpu().tolist(),\n",
    "        \"samples\": samples,\n",
    "        \"loss_tensor\": loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f51f7b-ecd4-48fc-b4be-3cda8aeb6fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'advantages': [0.0, 0.0],\n",
      " 'loss': -0.0,\n",
      " 'loss_tensor': tensor(-0., device='cuda:0', grad_fn=<NegBackward0>),\n",
      " 'pg_loss': -0.0,\n",
      " 'rewards': [0.0, 0.0],\n",
      " 'samples': [{'gen_len': 3, 'reward': 0.0, 'text': ' 14'},\n",
      "             {'gen_len': 3, 'reward': 0.0, 'text': ' 4 days'}]}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "stats = compute_grpo_loss(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    example=math_train[4],\n",
    "    device=device,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "pprint(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33683def-8030-4117-9441-7c581446debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_rlvr_grpo(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    math_data,\n",
    "    device,\n",
    "    steps=None,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    lr=1e-5,\n",
    "    checkpoint_every=50,\n",
    "    checkpoint_dir=\".\",\n",
    "    csv_log_path=None,\n",
    "\n",
    "):\n",
    "    if steps is None:\n",
    "        steps = len(math_data)\n",
    "\n",
    "    # Stage 1: initialize optimize\n",
    "    # (the model was already initialized outside the function)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    current_step = 0\n",
    "    if csv_log_path is None:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_log_path = f\"train_rlvr_grpo_metrics_{timestamp}.csv\"\n",
    "    csv_log_path = Path(csv_log_path)\n",
    "\n",
    "    try:\n",
    "        # Stage 2: Iterate over training steps\n",
    "        for step in range(steps):\n",
    "\n",
    "            # Stage 3: Reset loss gradient\n",
    "            # (it's best practice to do this at the beginning of each step)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_step = step + 1\n",
    "            example = math_data[step % len(math_data)]\n",
    "\n",
    "            # Stage 4: calculate GRPO loss\n",
    "            stats = compute_grpo_loss(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                example=example,\n",
    "                device=device,\n",
    "                num_rollouts=num_rollouts,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "\n",
    "            # Stage 5: Backward pass to calculate loss gradients\n",
    "            stats[\"loss_tensor\"].backward()\n",
    "\n",
    "            # Clip large gradients to improve training stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Stage 6: Update model weights using loss gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Stage 7: Collect rewards, response lengths, and losses\n",
    "            reward_avg = torch.tensor(stats[\"rewards\"]).mean().item()\n",
    "            step_tokens = sum(\n",
    "                sample[\"gen_len\"] for sample in stats[\"samples\"]\n",
    "            )\n",
    "            avg_response_len = (\n",
    "                step_tokens / len(stats[\"samples\"]) \n",
    "                if stats[\"samples\"] else 0.0\n",
    "            )\n",
    "            append_csv_metrics(\n",
    "                csv_log_path, current_step, steps, stats[\"loss\"],\n",
    "                reward_avg, avg_response_len,\n",
    "            )\n",
    "\n",
    "            # Print step metrics\n",
    "            print(\n",
    "                f\"[Step {current_step}/{steps}] \"\n",
    "                f\"loss={stats['loss']:.4f} \"\n",
    "                f\"reward_avg={reward_avg:.3f} \"\n",
    "                f\"avg_resp_len={avg_response_len:.1f}\"\n",
    "            )\n",
    "\n",
    "            # Sample outputs (every 10 steps) to check if model\n",
    "            # generates coherent text\n",
    "            if current_step % 10 == 0:\n",
    "                print(f\"[Step {current_step}] sample outputs\")\n",
    "                for i, sample in enumerate(stats[\"samples\"][:3]):\n",
    "                    text = sample[\"text\"].replace(\"\\n\", \"\\\\n\")\n",
    "                    print(\n",
    "                        f\"  {i+1}) reward={sample['reward']:.3f} \"\n",
    "                        f\"len={sample['gen_len']}: {text}\"\n",
    "                    )\n",
    "                print()\n",
    "\n",
    "            # Stage 8: Save model checkpoint\n",
    "            if checkpoint_every and current_step % checkpoint_every == 0:\n",
    "                ckpt_path = save_checkpoint(\n",
    "                    model=model,\n",
    "                    checkpoint_dir=checkpoint_dir,\n",
    "                    step=current_step,\n",
    "                )\n",
    "                print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "\n",
    "    # Save a model checkpoint if we interrupt the training early\n",
    "    except KeyboardInterrupt:\n",
    "        ckpt_path = save_checkpoint(\n",
    "            model=model,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            step=max(1, current_step),\n",
    "            suffix=\"interrupt\",\n",
    "        )\n",
    "        print(f\"\\nKeyboardInterrupt. Saved checkpoint to {ckpt_path}\")\n",
    "        return model\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_checkpoint(model, checkpoint_dir, step, suffix=\"\"):\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = f\"-{suffix}\" if suffix else \"\"\n",
    "    ckpt_path = (\n",
    "        checkpoint_dir /\n",
    "        f\"qwen3-0.6B-rlvr-grpo-step{step:05d}{suffix}.pth\"\n",
    "    )\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    return ckpt_path\n",
    "\n",
    "\n",
    "def append_csv_metrics(\n",
    "    csv_log_path,\n",
    "    step_idx,\n",
    "    total_steps,\n",
    "    loss,\n",
    "    reward_avg,\n",
    "    avg_response_len,\n",
    "):\n",
    "    if not csv_log_path.exists():\n",
    "        csv_log_path.write_text(\n",
    "            \"step,total_steps,loss,reward_avg,avg_response_len\\n\",\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "    with csv_log_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            f\"{step_idx},{total_steps},{loss:.6f},{reward_avg:.6f},\"\n",
    "            f\"{avg_response_len:.6f}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5baf6f-3087-4a1f-85e2-ef4be54942d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA GPU\n",
      "[Step 1/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=5.2\n",
      "[Step 2/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=5.8\n",
      "[Step 3/50] loss=3.5188 reward_avg=0.500 avg_resp_len=23.0\n",
      "[Step 4/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=1.5\n",
      "[Step 5/50] loss=14.2968 reward_avg=0.500 avg_resp_len=233.2\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00005.pth\n",
      "[Step 6/50] loss=1.5443 reward_avg=0.250 avg_resp_len=355.0\n",
      "[Step 7/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=494.8\n",
      "[Step 8/50] loss=3.2774 reward_avg=0.750 avg_resp_len=175.8\n",
      "[Step 9/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=248.8\n",
      "[Step 10/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=469.8\n",
      "[Step 10] sample outputs\n",
      "  1) reward=0.000 len=512:  To determine how much Tim should invest to reach \\$60,000 in 5 years with quarterly compounding at an annual interest rate of 7%, we can use the formula for compound interest:\\n\\n\\[\\nA = P \\left(1 + \\frac{r}{n}\\right)^{nt}\\n\\]\\n\\nWhere:\\n- \\( A \\) is the amount of money accumulated after \\( t \\) years, including interest.\\n- \\( P \\) is the principal amount (the initial amount of money).\\n- \\( r \\) is the annual interest rate (decimal).\\n- \\( n \\) is the number of times that interest is compounded per year.\\n- \\( t \\) is the time the money is invested for in years.\\n\\nGiven:\\n- \\( A = \\$60,000 \\)\\n- \\( r = 7\\% = 0.07 \\)\\n- \\( n = 4 \\) (since the interest is compounded quarterly)\\n- \\( t = 5 \\) years\\n\\nWe need to solve for \\( P \\):\\n\\n\\[\\n60,000 = P \\left(1 + \\frac{0.07}{4}\\right)^{4 \\times 5}\\n\\]\\n\\nFirst, calculate the interest rate per compounding period:\\n\\n\\[\\n\\frac{0.07}{4} = 0.0175\\n\\]\\n\\nNext, calculate the total number of compounding periods:\\n\\n\\[\\n4 \\times 5 = 20\\n\\]\\n\\nNow, plug these values into the formula:\\n\\n\\[\\n60,000 = P \\left(1 + 0.0175\\right)^{20}\\n\\]\\n\\nSimplify the expression inside the parentheses:\\n\\n\\[\\n1 + 0.0175 = 1.0175\\n\\]\\n\\nSo the equation becomes:\\n\\n\\[\\n60,000 = P \\times (1.0175)^{20}\\n\\]\\n\\nCalculate \\( (1.0175)^{20} \\):\\n\\n\\[\\n(1.0175)^{20} \\approx 1.4017\\n\\]\\n\\nNow, solve for \\( P \\):\\n\\n\\[\\nP = \\frac{60,000}{1.4017} \\approx 42,832.34\\n\\]\\n\\nRounding to the nearest dollar:\\n\\n\\[\\nP \\approx \n",
      "  2) reward=0.000 len=461:  To determine how much Tim should invest to reach \\$60,000 in 5 years with a quarterly compound interest rate of 7%, we can use the compound interest formula:\\n\\n\\[\\nA = P \\left(1 + \\frac{r}{n}\\right)^{nt}\\n\\]\\n\\nWhere:\\n- \\( A \\) is the amount of money accumulated after \\( t \\) years, including interest.\\n- \\( P \\) is the principal amount (the initial amount of money).\\n- \\( r \\) is the annual interest rate (decimal).\\n- \\( n \\) is the number of times that interest is compounded per year.\\n- \\( t \\) is the time the money is invested for in years.\\n\\nGiven:\\n- \\( A = \\$60,000 \\)\\n- \\( r = 7\\% = 0.07 \\)\\n- \\( n = 4 \\) (since the interest is compounded quarterly)\\n- \\( t = 5 \\) years\\n\\nWe need to solve for \\( P \\):\\n\\n\\[\\n60,000 = P \\left(1 + \\frac{0.07}{4}\\right)^{4 \\times 5}\\n\\]\\n\\nFirst, calculate the interest rate per period:\\n\\n\\[\\n\\frac{0.07}{4} = 0.0175\\n\\]\\n\\nNext, calculate the exponent:\\n\\n\\[\\n4 \\times 5 = 20\\n\\]\\n\\nNow, calculate the term inside the parentheses:\\n\\n\\[\\n1 + 0.0175 = 1.0175\\n\\]\\n\\nRaise this to the power of 20:\\n\\n\\[\\n1.0175^{20} \\approx 1.4164\\n\\]\\n\\nNow, solve for \\( P \\):\\n\\n\\[\\nP = \\frac{60,000}{1.4164} \\approx 42,105.18\\n\\]\\n\\nRounding to the nearest dollar:\\n\\n\\[\\nP \\approx \\$42,105\\n\\]\\n\\nSo, Tim should invest approximately **\\$42,105** to reach \\$60,000 in 5 years.\n",
      "  3) reward=0.000 len=451:  To determine how much Tim should invest to reach \\$60,000 in 5 years with a quarterly compound interest rate of 7%, we can use the compound interest formula:\\n\\n\\[\\nA = P \\left(1 + \\frac{r}{n}\\right)^{nt}\\n\\]\\n\\nWhere:\\n- \\( A \\) is the amount of money accumulated after \\( t \\) years, including interest.\\n- \\( P \\) is the principal amount (the initial amount of money).\\n- \\( r \\) is the annual interest rate (decimal).\\n- \\( n \\) is the number of times that interest is compounded per year.\\n- \\( t \\) is the time the money is invested for in years.\\n\\nGiven:\\n- \\( A = \\$60,000 \\)\\n- \\( r = 7\\% = 0.07 \\)\\n- \\( n = 4 \\) (since the interest is compounded quarterly)\\n- \\( t = 5 \\) years\\n\\nWe need to solve for \\( P \\):\\n\\n\\[\\n60,000 = P \\left(1 + \\frac{0.07}{4}\\right)^{4 \\times 5}\\n\\]\\n\\nFirst, calculate the interest rate per quarter:\\n\\n\\[\\n\\frac{0.07}{4} = 0.0175\\n\\]\\n\\nNext, calculate the exponent:\\n\\n\\[\\n4 \\times 5 = 20\\n\\]\\n\\nNow, calculate the expression inside the parentheses:\\n\\n\\[\\n1 + 0.0175 = 1.0175\\n\\]\\n\\nRaise this to the power of 20:\\n\\n\\[\\n1.0175^{20} \\approx 1.4026\\n\\]\\n\\nNow, solve for \\( P \\):\\n\\n\\[\\nP = \\frac{60,000}{1.4026} \\approx 42,817.34\\n\\]\\n\\nRounding to the nearest dollar:\\n\\n\\[\\nP \\approx \\$42,817\\n\\]\\n\\nTherefore, Tim should invest:\\n\\n\\[\\n\\boxed{42817}\\n\\]\n",
      "\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00010.pth\n",
      "[Step 11/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=261.8\n",
      "[Step 12/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=342.0\n",
      "[Step 13/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=212.0\n",
      "[Step 14/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=256.8\n",
      "[Step 15/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=185.0\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00015.pth\n",
      "[Step 16/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=263.8\n",
      "[Step 17/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=512.0\n",
      "[Step 18/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=321.5\n",
      "[Step 19/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=324.5\n",
      "[Step 20/50] loss=-0.7560 reward_avg=0.500 avg_resp_len=476.0\n",
      "[Step 20] sample outputs\n",
      "  1) reward=1.000 len=502:  To solve this problem, we'll first determine the rate at which the faucets fill the tub and then use that rate to find out how long it takes for six faucets to fill a 25-gallon tub.\\n\\n### Step 1: Determine the rate of one faucet\\nWe know that three faucets fill a 100-gallon tub in 6 minutes. Therefore, the combined rate of three faucets is:\\n\\[\\n\\text{Rate of three faucets} = \\frac{100 \\text{ gallons}}{6 \\text{ minutes}} = \\frac{50}{3} \\text{ gallons per minute}\\n\\]\\nThis means that one faucet fills:\\n\\[\\n\\text{Rate of one faucet} = \\frac{50}{3} \\div 3 = \\frac{50}{9} \\text{ gallons per minute}\\n\\]\\n\\n### Step 2: Determine the rate of six faucets\\nIf one faucet fills \\(\\frac{50}{9}\\) gallons per minute, then six faucets will fill:\\n\\[\\n\\text{Rate of six faucets} = 6 \\times \\frac{50}{9} = \\frac{300}{9} = \\frac{100}{3} \\text{ gallons per minute}\\n\\]\\n\\n### Step 3: Calculate the time to fill a 25-gallon tub\\nWe need to find the time \\(t\\) it takes for six faucets to fill a 25-gallon tub. Using the rate of six faucets, we can set up the equation:\\n\\[\\n\\frac{100}{3} \\times t = 25\\n\\]\\nSolving for \\(t\\):\\n\\[\\nt = 25 \\times \\frac{3}{100} = \\frac{75}{100} = \\frac{3}{4} \\text{ minutes}\\n\\]\\n\\n### Step 4: Convert the time to seconds\\nSince there are 60 seconds in a minute, we convert \\(\\frac{3}{4}\\) minutes to seconds:\\n\\[\\nt = \\frac{3}{4} \\times 60 = 45 \\text{ seconds}\\n\\]\\n\\n### Final Answer\\nThe time it takes for six faucets to fill a 25-gallon tub is:\\n\\[\\n\\boxed{45}\\n\\]\n",
      "  2) reward=1.000 len=500:  To solve this problem, we'll first determine the rate at which the faucets fill the tub, and then use that rate to find out how long it takes for six faucets to fill a 25-gallon tub.\\n\\n### Step 1: Determine the rate of one faucet\\nThree faucets fill a 100-gallon tub in 6 minutes. So, the total volume filled by three faucets in 6 minutes is 100 gallons.\\n\\nFirst, find the rate of one faucet:\\n\\[\\n\\text{Rate of one faucet} = \\frac{100 \\text{ gallons}}{3 \\text{ faucets} \\times 6 \\text{ minutes}} = \\frac{100}{18} \\text{ gallons per minute per faucet}\\n\\]\\n\\n### Step 2: Determine the rate of six faucets\\nNow, we need to find the rate of six faucets. Since the rate is constant, we multiply the rate of one faucet by six:\\n\\[\\n\\text{Rate of six faucets} = 6 \\times \\frac{100}{18} = \\frac{600}{18} = \\frac{100}{3} \\text{ gallons per minute}\\n\\]\\n\\n### Step 3: Calculate the time to fill a 25-gallon tub\\nWe need to find out how long it takes for six faucets to fill a 25-gallon tub. First, convert the time from minutes to seconds:\\n\\[\\n6 \\text{ minutes} = 6 \\times 60 = 360 \\text{ seconds}\\n\\]\\n\\nNow, use the rate of six faucets to find the time:\\n\\[\\n\\text{Time} = \\frac{\\text{Volume}}{\\text{Rate}} = \\frac{25 \\text{ gallons}}{\\frac{100}{3} \\text{ gallons per minute}} = 25 \\times \\frac{3}{100} = \\frac{75}{100} = 0.75 \\text{ minutes}\\n\\]\\n\\nConvert 0.75 minutes to seconds:\\n\\[\\n0.75 \\text{ minutes} = 0.75 \\times 60 = 45 \\text{ seconds}\\n\\]\\n\\n### Final Answer:\\n\\[\\n\\boxed{45}\\n\\]\n",
      "  3) reward=0.000 len=470:  To solve this problem, we'll first determine the rate at which the faucets fill the tub, and then use that rate to find out how long it takes for six faucets to fill a 25-gallon tub.\\n\\n### Step 1: Determine the rate of one faucet\\nThree faucets fill a 100-gallon tub in 6 minutes. First, find the total volume filled by one faucet in 6 minutes.\\n\\nSince three faucets fill 100 gallons in 6 minutes, one faucet fills:\\n\\[\\n\\frac{100 \\text{ gallons}}{3 \\text{ faucets}} = \\frac{100}{3} \\text{ gallons per faucet per minute}\\n\\]\\n\\n### Step 2: Calculate the rate for six faucets\\nNow, we need to find out how long it takes for six faucets to fill a 25-gallon tub. We know that one faucet fills \\(\\frac{100}{3}\\) gallons per minute. Therefore, six faucets will fill:\\n\\[\\n6 \\times \\frac{100}{3} = 200 \\text{ gallons per minute}\\n\\]\\n\\n### Step 3: Determine the time to fill 25 gallons\\nWe need to find the time \\(t\\) in minutes it takes for six faucets to fill 25 gallons. Using the rate of 200 gallons per minute, we have:\\n\\[\\n200 \\text{ gallons per minute} \\times t = 25 \\text{ gallons}\\n\\]\\nSolving for \\(t\\):\\n\\[\\nt = \\frac{25}{200} = \\frac{1}{8} \\text{ minutes}\\n\\]\\n\\n### Step 4: Convert minutes to seconds\\nSince there are 60 seconds in a minute, we convert \\(\\frac{1}{8}\\) minutes to seconds:\\n\\[\\n\\frac{1}{8} \\text{ minutes} \\times 60 \\text{ seconds per minute} = 7.5 \\text{ seconds}\\n\\]\\n\\n### Final Answer\\nThe time it takes for six faucets to fill a 25-gallon tub is:\\n\\[\\n\\boxed{7.5}\\n\\]\n",
      "\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00020.pth\n",
      "[Step 21/50] loss=-0.0000 reward_avg=1.000 avg_resp_len=386.2\n",
      "[Step 22/50] loss=-5.7331 reward_avg=0.250 avg_resp_len=366.0\n",
      "[Step 23/50] loss=-20.0540 reward_avg=0.500 avg_resp_len=457.0\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_rlvr_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    math_data=math_train,\n",
    "    device=device,\n",
    "    steps=50,\n",
    "    num_rollouts=4,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    lr=1e-5,\n",
    "    checkpoint_every=5,\n",
    "    checkpoint_dir=\".\",\n",
    "    csv_log_path=\"train_rlvr_grpo_metrics.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ff9a3-fac5-44f2-aacf-7ad276981c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39cf42-491b-498d-b0da-23a45817c4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e3645-d8f3-4002-8acc-644ddc64d854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11702b8a-05e6-4efc-b0e1-8ba98ec1fbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb721a-cf7a-43e6-9879-0c718de4f215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d0569-5854-4e48-9a0b-79b11fc585dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cc0c4-9f92-4099-b37f-1f1deace7742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba0e1c-e117-4fff-9628-a303a0412dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73c9f2-090a-404f-b9aa-c062f637b2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8264ab-c40b-488b-a6cb-f8ec1141cda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca1882a-6d1a-41d3-96a2-2c2dd7f62eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a099aad4-d151-4a35-be91-7722c4736b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "# MODEL_NAME = \"Qwen/qwen3-0.6B-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6860cfa-17e8-4120-8dcb-28df767d247f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1923c6aa62e940299fb83449423b492c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac4c25f640c4e8fbe0fc246c14b6706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04510c25b47c4f8e95bf009933043670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=0,\n",
    "# )\n",
    "# # reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "# #     MODEL_NAME,\n",
    "# #     attn_implementation=\"flash_attention_2\",\n",
    "# #     torch_dtype=torch.bfloat16,\n",
    "# #     device_map=0,\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aff8f3-e790-41c3-bb63-956557b69035",
   "metadata": {},
   "source": [
    "- Hmm yeah this is like a non-trivial amount of tooling with either repo\n",
    "- Raschka's seems friendlier, but more complex than I would want in some areas for sure.\n",
    "- Can I achieve my hacking goals here using Raschka's code?\n",
    "- Seems like it comes down to if I can make sense of the policy gradient part of his code and modify it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb7869-c152-48f9-9104-68561805fd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02071b95-008c-418e-8559-f815bd7d7b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfab81-86af-460c-a589-b47d05836223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48a1e9-0c05-4272-b724-92916a86afc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bbd0fc-d76d-4d52-8bd0-7b20b48fa9dc",
   "metadata": {},
   "source": [
    "## Stephen RL Hacking 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091a95c-4b1c-4788-8d26-b8b1df34d7d2",
   "metadata": {},
   "source": [
    "Hack in this direction as long as it makes sense: \n",
    "\n",
    "- [ ]  Setup a basic LLM (maybe QWEN 0.6B)\n",
    "- [ ]  A math dataset\n",
    "- [ ]  Run various policy gradient methods to teach the model to reason (GRPO, Raschka’s simpler GRPO, Dr. GRPO, PPO, Vanilla Policy gradient, maybe DPO, maybe RPOO)\n",
    "- [ ]  How do implementations perform and vary? Do components make sense? Can we replicated some version of the “aha moment”? (although that’s apparently not an emergent RL property?) Does the way the math is setup make sense? does Karpathy’s simplified explanation make it easier? Would demoing on or more of these algorithms on in a game setting make more sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0106e1-9842-435b-8bb1-20f995053ecb",
   "metadata": {},
   "source": [
    "- https://github.com/rasbt/reasoning-from-scratch/blob/main/ch06/01_main-chapter-code/ch06_main.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14581734-61a7-497c-9d82-10f083dbc798",
   "metadata": {},
   "source": [
    "- Hmm why is Raschka using his own QWEN implementation, seems like more complexity than we want?\n",
    "- What do they do for the model here: https://github.com/McGill-NLP/nano-aha-moment/blob/main/nano_r1.ipynb ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed1b352-91eb-4253-b879-4ac9bef994f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the environment variables for HuggingFace\n",
    "# This is done to ensure that the cache directory for HuggingFace is set to a specific location,\n",
    "# preventing the storage from being overwhelmed with model files and other data.\n",
    "SCRATCH = Path.home() / \"scratch\"\n",
    "os.environ[\"HF_HOME\"] = str(SCRATCH / \"hf_home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abeeccf-fd86-47a9-aed8-18647242644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "import deepspeed\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from deepspeed import DeepSpeedEngine\n",
    "from tqdm import trange\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import wandb\n",
    "from utils import (\n",
    "    compute_token_log_probs,\n",
    "    dump_episodes,\n",
    "    evaluate_on_test_set,\n",
    "    find_free_port,\n",
    "    find_last_checkpoint,\n",
    "    prepare_model_inputs,\n",
    "    load_model_into_vllm\n",
    ")\n",
    "\n",
    "# Needed to stop DeepSpeed from complaining\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31655d6e-6875-45d1-929c-154bacbcb98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs and Checkpoints will be saved to: /home/stephen/scratch/deepseek_r1z_hackathon/r1-zero\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-06B\"\n",
    "\n",
    "MODEL_CHAT_NAME = MODEL_NAME + \"-Instruct\"\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "\n",
    "# Total number of training iterations\n",
    "NUM_ITERATIONS = 1000\n",
    "# Number of episodes to collect per iteration for training\n",
    "EPISODES_PER_ITERATION = 64\n",
    "# Number of responses to generate for each input prompt (i.e. group size in GRPO)\n",
    "GENERATIONS_PER_SAMPLE = 4\n",
    "# Controls how much the policy can deviate from the reference model\n",
    "KL_COEFFICIENT = 0.001\n",
    "\n",
    "# Training hyperparameters\n",
    "# Batch size for each GPU device during training\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "# Learning rate for model updates\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "# Sampling parameters\n",
    "# Maximum number of tokens to generate in each response\n",
    "MAX_RESPONSE_TOKENS = 1024\n",
    "# Controls randomness in generation (higher = more random)\n",
    "TEMPERATURE = 1.0\n",
    "# Nucleus sampling parameter (1.0 = disabled)\n",
    "TOP_P = 1.0\n",
    "# Top-k sampling parameter (-1 = disabled)\n",
    "TOP_K = -1  # no top k\n",
    "\n",
    "# DeepSpeed configuration\n",
    "# DeepSpeed config for the policy model\n",
    "deepspeed_config = {\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\"stage\": 2, \"overlap_comm\": False},\n",
    "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
    "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": LEARNING_RATE,\n",
    "            \"betas\": (0.9, 0.999),\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"torch_adam\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "# DeepSpeed config for the reference model\n",
    "ref_deepspeed_config = {\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    # Note that we don't train the reference model\n",
    "    # These are just for compatibility with DeepSpeed.\n",
    "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
    "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
    "}\n",
    "\n",
    "RUN_NAME = \"r1-zero\"\n",
    "EXP_DIR = SCRATCH / \"deepseek_r1z_hackathon\" / RUN_NAME\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Logs and Checkpoints will be saved to: {EXP_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b36976b-af4f-427c-be12-84fee14d4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = (\n",
    "    \"You are a helpful assistant. You first think about the reasoning process in the mind \"\n",
    "    \"and then provide the user with the answer.\"\n",
    ")\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Using the numbers {numbers}, create an equation that equals {target}. \"\n",
    "    \"You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. \"\n",
    "    \"Show your work in <think> </think> tags. And return the final equation and answer in \"\n",
    "    \"<answer> </answer> tags, for example <answer>(1 + 2) / (3 * 5)</answer>.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58f7d8a-2a26-4d69-ac01-ec2365591320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433bc392e2f24dda8999f2697c02c267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa44010aa7ba4f80965a95de2d481348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d91377115894eba83b3d19d4e1c621f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a1740cf7134220806cfe54f243328a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a85ac18e6241f781dd9b3138481262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c2bc4a2ea44616a63d1a57c6164422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d02e0f5138a4fd5819979c11b8c93c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf39d7a0e94408aa558cbebe32e9760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc938a0c543a45dbac029bfe1bf10668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24e0743ad8e444d9821abeff234a145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119ae4fe94674ccebacfd0ce196b69df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef106af4b9324c77a438a1ad8c75f35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(489864, 500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process dataset\n",
    "def preprocess_example(example: Dict[str, Any]):\n",
    "    numbers: List[int] = example[\"nums\"]\n",
    "    target: int = example[\"target\"]\n",
    "\n",
    "    prefix = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(numbers=numbers, target=target)},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prefix, tokenize=True, continue_final_message=True\n",
    "    )\n",
    "    prompt = tokenizer.decode(\n",
    "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"input_ids\": input_ids}\n",
    "\n",
    "# Note that the base model and \"instruct\" model have different eos token. \n",
    "# Here we make sure to use the correct one.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHAT_NAME)\n",
    "EOS_TOKEN_ID = AutoTokenizer.from_pretrained(MODEL_NAME).eos_token_id\n",
    "EOS_TOKEN = tokenizer.convert_ids_to_tokens(EOS_TOKEN_ID)\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "dataset = dataset.map(preprocess_example, num_proc=6)\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = dataset.train_test_split(test_size=500, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68837c37-dd16-46ef-aae6-3d5075f94b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  43\n",
      "Available Numbers:  [4, 27, 12]\n"
     ]
    }
   ],
   "source": [
    "print(\"Target: \", train_dataset[0][\"target\"])\n",
    "print(\"Available Numbers: \", train_dataset[0][\"nums\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d39008a1-b09d-41d7-b716-716d53375e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant. You first think about the reasoning process in the mind and then provide the user with the answer.<|im_end|>\n",
      "<|im_start|>user\n",
      "Using the numbers [4, 27, 12], create an equation that equals 43. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer>(1 + 2) / (3 * 5)</answer>.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let me solve this step by step.\n",
      "<think>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9908a61d-0cdb-4edd-a34a-9089ef05fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completion: str) -> float:\n",
    "    \"\"\"\n",
    "    Format: <think>...</think>\\n</answer>...</answer>\n",
    "\n",
    "    Also checks that the content within <answer>...</answer> conforms to a\n",
    "    specified pattern (only digits, + - * / ( ) . and whitespace).\n",
    "\n",
    "    Args:\n",
    "        completion (str): Generated output\n",
    "\n",
    "    Returns:\n",
    "        float: Reward score\n",
    "    \"\"\"\n",
    "    # Define the allowed pattern (only numbers, +, -, *, /, (, ), ., and whitespace)\n",
    "    allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "\n",
    "    try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled \n",
    "        # for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion\n",
    "\n",
    "        # Strip EOS token if present\n",
    "        if completion.endswith(EOS_TOKEN):\n",
    "            completion = completion[:-len(EOS_TOKEN)]\n",
    "\n",
    "        # Check if the format is correct\n",
    "        # Pattern means:\n",
    "        # 1) <think>...contents not including other <think> tags...</think>\n",
    "        # 2) \\n\n",
    "        # 3) <answer>...anything...</answer>\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            # Format is incorrect\n",
    "            return 0.0\n",
    "        else:\n",
    "            # Extract the content inside <answer>...</answer>\n",
    "            answer_content = match.group(2).strip()\n",
    "\n",
    "            # Check if answer content matches the allowed pattern\n",
    "            if not re.match(allowed_pattern, answer_content):\n",
    "                # If it doesn't match, reward is 0.5\n",
    "                return 0.5\n",
    "            else:\n",
    "                # If both format and pattern are correct, reward is 1\n",
    "                return 1.0\n",
    "    except Exception:\n",
    "        # Any error leads to 0 reward\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def equation_reward_func(completion: str, nums: List[int], target: int) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates completion based on mathematical correctness of the answer\n",
    "\n",
    "    Args:\n",
    "        completion (str): Generated output\n",
    "        target (str): Expected answer\n",
    "        nums (list): Available numbers to use in the equation\n",
    "\n",
    "    Returns:\n",
    "        float: Reward score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the format is correct\n",
    "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "        if match is None:\n",
    "            return 0.0\n",
    "        # Extract the \"answer\" part from the completion\n",
    "        equation = match.group(1).strip()\n",
    "        # Extract all numbers from the equation\n",
    "        used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n",
    "\n",
    "        # Check if all numbers are used exactly once\n",
    "        if sorted(used_numbers) != sorted(nums):\n",
    "            return 0.0\n",
    "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
    "        allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "        if not re.match(allowed_pattern, equation):\n",
    "            return 0.0\n",
    "\n",
    "        # Evaluate the equation with restricted globals and locals\n",
    "        result = eval(equation, {\"__builtins__\": None}, {})\n",
    "        # Check if the equation is correct and matches the ground truth\n",
    "        if abs(float(result) - float(target)) < 1e-5:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    except Exception:\n",
    "        # If evaluation fails, reward is 0\n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "def compute_reward(completion: str, sample: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:\n",
    "    nums = sample[\"nums\"]\n",
    "    target = sample[\"target\"]\n",
    "\n",
    "    format_reward = format_reward_func(completion)\n",
    "    equation_reward = equation_reward_func(\n",
    "        completion=completion, nums=nums, target=target\n",
    "    )\n",
    "\n",
    "    reward = format_reward + equation_reward\n",
    "\n",
    "    metrics = {\n",
    "        \"format_reward\": format_reward,\n",
    "        \"equation_reward\": equation_reward,\n",
    "    }   \n",
    "\n",
    "    return reward, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ffc195-634f-4a3b-90a3-ff722673c0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <think> is prefilled in the prompt. So, repeating it in the completion would be incorret.\n",
    "format_reward_func(\"<think>I think the answer is </think>\\n<answer>1+2</answer>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "978dcc65-2055-4e9a-b814-9fe92d038dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_reward_func(\"I think the answer is </think>\\n<answer>1+2</answer>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "937db8e5-8e28-4604-80c3-c041ce3a474a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_reward_func(\"<think>I think the<think>and even more</think> answer is </think>\\n<answer>1+2</answer>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "410d819a-99b2-42f0-b7c6-eedb8f9c9207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equation_reward_func(\"I think the answer is </think>\\n<answer>1+2+2</answer>\", [1,2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a309e786-4a9d-460f-9f18-572f5cf17868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_episodes(\n",
    "    samples: List[Dict[str, Any]],\n",
    "    all_generations: List[List[int]],\n",
    "    all_finish_reasons: List[str],\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process model generations and calculate rewards for training episodes.\n",
    "\n",
    "    This function processes generated responses and calculates rewards for training episodes by:\n",
    "    1. Grouping generations by sample (GENERATIONS_PER_SAMPLE responses per input)\n",
    "    2. Computing rewards and advantages for each response\n",
    "    3. Processing response tokens\n",
    "\n",
    "    Args:\n",
    "        samples: List of input samples, each containing:\n",
    "            - input_ids: List[int], tokenized input prompt\n",
    "            - nums: List[int], numbers to use in equation\n",
    "            - target: int, target value for equation\n",
    "        all_generations: List of token ID sequences for each generated response\n",
    "        all_finish_reasons: List of finish reasons for each generation (\"stop\" or other)\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        1. Dictionary with processed data for training:\n",
    "            - all_query_token_ids: List[List[int]], input token IDs repeated for each generation\n",
    "            - all_response_token_ids: List[List[int]], response token IDs with EOS tokens added\n",
    "            - all_advantages: List[List[float]], advantage values repeated for each token\n",
    "        2. Dictionary with generation statistics:\n",
    "            - response_lengths: List[int], lengths of generated responses\n",
    "            - rewards: List[float], raw reward values\n",
    "            - non_stop_rate: List[bool], whether each generation ended naturally\n",
    "            - reward_metrics/*: Various reward component metrics\n",
    "\n",
    "    Example:\n",
    "        >>> samples = [{\"input_ids\": [1,2,3], \"nums\": [1,2,3], \"target\": 6}]\n",
    "        >>> generations = [[4,5, EOS_TOKEN_ID], [6,7], [8,9, EOS_TOKEN_ID]]  # 3 generations per sample\n",
    "        >>> finish_reasons = [\"stop\", \"length\", \"stop\"]\n",
    "        >>> episodes, stats = create_training_episodes(samples, generations, finish_reasons)\n",
    "        >>> episodes\n",
    "        {\n",
    "            'all_query_token_ids': [[1,2,3], [1,2,3], [1,2,3]],\n",
    "            'all_response_token_ids': [[4,5,EOS_TOKEN_ID], [6,7], [8,9,EOS_TOKEN_ID]],\n",
    "            'all_advantages': [[0.5,0.5,0.5], [-1.0,-1.0], [0.5,0.5,0.5]]\n",
    "        }\n",
    "    \"\"\"\n",
    "    assert len(all_generations) == len(all_finish_reasons)\n",
    "    assert len(all_generations) == len(samples) * GENERATIONS_PER_SAMPLE\n",
    "\n",
    "    # Process responses and calculate rewards\n",
    "    groups = [\n",
    "        list(range(i, i + GENERATIONS_PER_SAMPLE))\n",
    "        for i in range(0, len(all_generations), GENERATIONS_PER_SAMPLE)\n",
    "    ]  # example: [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "\n",
    "    all_query_token_ids, all_responses_token_ids, all_advantages = [], [], []\n",
    "\n",
    "    stats = {\n",
    "        \"response_lengths\": [],\n",
    "        \"rewards\": [],\n",
    "        \"non_stop_rate\": [],\n",
    "    }\n",
    "\n",
    "    for sample, group_indices in zip(samples, groups):\n",
    "        finish_reasons = [all_finish_reasons[i] for i in group_indices]\n",
    "        response_token_ids = [all_generations[i] for i in group_indices]\n",
    "        responses = tokenizer.batch_decode(response_token_ids, skip_special_tokens=False)\n",
    "\n",
    "        rewards_and_metrics = [compute_reward(resp, sample) for resp in responses]\n",
    "        rewards, reward_metrics = zip(*rewards_and_metrics)\n",
    "\n",
    "        rewards = np.array(rewards) # [group_size]\n",
    "        response_advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4)\n",
    "        \n",
    "        advantages = [\n",
    "            [resp_adv] * len(resp) \n",
    "            for resp_adv, resp in zip(response_advantages, response_token_ids)\n",
    "        ]\n",
    "\n",
    "        all_query_token_ids.extend([sample[\"input_ids\"]] * GENERATIONS_PER_SAMPLE)\n",
    "        all_responses_token_ids.extend(response_token_ids)\n",
    "        all_advantages.extend(advantages)\n",
    "\n",
    "        stats[\"rewards\"].extend(rewards)\n",
    "        stats[\"non_stop_rate\"].extend([fr != \"stop\" for fr in finish_reasons])\n",
    "        stats[\"response_lengths\"].extend([len(ids) for ids in response_token_ids])\n",
    "        for rm in reward_metrics:\n",
    "            for k, v in rm.items():\n",
    "                stats.setdefault(f\"reward_metrics/{k}\", []).append(v)\n",
    "\n",
    "    episodes = {\n",
    "        \"all_query_token_ids\": all_query_token_ids,\n",
    "        \"all_response_token_ids\": all_responses_token_ids,\n",
    "        \"all_advantages\": all_advantages,\n",
    "    }\n",
    "\n",
    "    return episodes, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a661120-2da6-43bf-aa51-ea5c1720c183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_query_token_ids': [[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]],\n",
       " 'all_response_token_ids': [[4, 5, 22, 33], [6, 7], [8, 9, 11], [10, 11]],\n",
       " 'all_advantages': [[np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0)],\n",
       "  [np.float64(0.0), np.float64(0.0)],\n",
       "  [np.float64(0.0), np.float64(0.0), np.float64(0.0)],\n",
       "  [np.float64(0.0), np.float64(0.0)]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_0 = {\n",
    "    \"sample\": {\"input_ids\": [1,2,3], \"nums\": [1,2,3], \"target\": 6},\n",
    "    \"generations\": [[4,5, 22, 33], [6,7], [8,9, 11], [10,11]],\n",
    "    \"finish_reasons\": [\"stop\", \"length\", \"stop\", \"stop\"]\n",
    "}\n",
    "\n",
    "case = case_0\n",
    "episodes, stats = create_training_episodes([case[\"sample\"]], case[\"generations\"], case[\"finish_reasons\"])\n",
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b034f414-9f31-40a8-b732-1e35803fcf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_query_token_ids': [[33, 44], [33, 44], [33, 44], [33, 44]],\n",
       " 'all_response_token_ids': [[1, 2], [3, 4], [5, 6], [7, 8]],\n",
       " 'all_advantages': [[np.float64(0.0), np.float64(0.0)],\n",
       "  [np.float64(0.0), np.float64(0.0)],\n",
       "  [np.float64(0.0), np.float64(0.0)],\n",
       "  [np.float64(0.0), np.float64(0.0)]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_1 = {\n",
    "    \"sample\": {\"input_ids\": [33, 44], \"nums\": [11, 7, 8], \"target\": 26},\n",
    "    \"generations\": [[1,2], [3,4], [5,6], [7,8]],\n",
    "    \"finish_reasons\": [\"stop\", \"stop\", \"length\", \"stop\"]\n",
    "}\n",
    "case = case_1\n",
    "episodes, stats = create_training_episodes([case[\"sample\"]], case[\"generations\"], case[\"finish_reasons\"])\n",
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f9941d2-62be-4d05-97ef-b4c301ab93b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_query_token_ids': [[9, 8, 7, 6, 5, 4],\n",
       "  [9, 8, 7, 6, 5, 4],\n",
       "  [9, 8, 7, 6, 5, 4],\n",
       "  [9, 8, 7, 6, 5, 4]],\n",
       " 'all_response_token_ids': [[9, 10], [11, 12], [13, 14], [15, 16]],\n",
       " 'all_advantages': [[np.float64(0.0), np.float64(0.0)],\n",
       "  [np.float64(0.0), np.float64(0.0)],\n",
       "  [np.float64(0.0), np.float64(0.0)],\n",
       "  [np.float64(0.0), np.float64(0.0)]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_2 = {\n",
    "    \"sample\": {\"input_ids\": [9, 8, 7, 6, 5, 4], \"nums\": [1,2,3,4], \"target\": 10},\n",
    "    \"generations\": [[9,10], [11,12], [13,14], [15,16]],\n",
    "    \"finish_reasons\": [\"length\", \"length\", \"stop\", \"stop\"]\n",
    "}\n",
    "case = case_2\n",
    "episodes, stats = create_training_episodes([case[\"sample\"]], case[\"generations\"], case[\"finish_reasons\"])\n",
    "episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb4055-a52d-4d34-8dba-04095b71fa42",
   "metadata": {},
   "source": [
    "Hmm kinda guessing the Rascka is doing \"fully online\", on learning step per sample - that's why he's not worried about the quotient stuuuuf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc9a2385-a221-42b3-9b13-6476cfb5cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pg_loss(\n",
    "    policy_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
    "    reference_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
    "    batch: Dict[str, torch.Tensor],\n",
    "    total_response_len: int,\n",
    ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute the policy gradient loss with KL penalty between policy and reference models.\n",
    "\n",
    "    This function:\n",
    "    1. Computes log probabilities for both policy and reference models\n",
    "    2. Calculates KL divergence penalty between the models\n",
    "    3. Computes policy gradient loss using advantages\n",
    "    4. Combines the losses with KL coefficient\n",
    "\n",
    "    Args:\n",
    "        policy_model: The model being trained\n",
    "        reference_model: The reference model for KL penalty calculation\n",
    "        batch: Dictionary containing:\n",
    "            - input_ids: Tensor of shape [batch_size, seq_len]\n",
    "            - attention_mask: Tensor of shape [batch_size, seq_len]\n",
    "            - labels: Tensor of shape [batch_size, seq_len] with -100 for ignored positions\n",
    "            - advantages: Tensor of shape [batch_size, seq_len]\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - loss: Combined policy gradient and KL penalty loss (scalar tensor)\n",
    "            - metrics: Dictionary with detailed loss components:\n",
    "                - policy_loss: Pure policy gradient loss\n",
    "                - kl_penalty: KL divergence penalty\n",
    "                - entropy: Policy entropy\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]  # [batch_size, seq_len]\n",
    "    attention_mask = batch[\"attention_mask\"]  # [batch_size, seq_len]\n",
    "    labels = batch[\"labels\"]  # [batch_size, seq_len]\n",
    "    advantages = batch[\"advantages\"]  # [batch_size, seq_len]\n",
    "\n",
    "    model_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    labels_mask = (labels[..., 1:] != -100).float()  # [batch_size, seq_len-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_logps = compute_token_log_probs(\n",
    "            reference_model, model_inputs, TEMPERATURE\n",
    "        )  # [batch_size, seq_len-1]\n",
    "\n",
    "    logps = compute_token_log_probs(policy_model, model_inputs, TEMPERATURE)  # [batch_size, seq_len-1]\n",
    "\n",
    "    kl_penalty = torch.exp(ref_logps - logps) - (ref_logps - logps) - 1  # [batch_size, seq_len-1]\n",
    "    kl_penalty = kl_penalty * labels_mask  # [batch_size, seq_len-1]\n",
    "\n",
    "    entropy = -logps.sum() / labels_mask.sum()  # scalar\n",
    "\n",
    "    policy_loss = -logps * advantages[..., 1:]  # [batch_size, seq_len-1]\n",
    "    policy_loss = policy_loss * labels_mask  # [batch_size, seq_len-1]\n",
    "\n",
    "    loss = (policy_loss + KL_COEFFICIENT * kl_penalty).sum() / total_response_len  # scalar\n",
    "\n",
    "    metrics = {\n",
    "        \"policy_loss\": policy_loss.sum().item() / total_response_len,\n",
    "        \"kl_penalty\": kl_penalty.sum().item() / total_response_len,\n",
    "        \"entropy\": entropy.item() / total_response_len,\n",
    "    }\n",
    "\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53b8aab3-601d-4ff6-b92b-4fa0d7803145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e296f64d614548618b881370f09e3b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/683 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8888b4a5e54642e281c38226b1b8bf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002028cba3bb4e45a99d5fa1ce900846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e242603d1f4d31bd666e9ee0023dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019a72f3927646dc926281db56d8344f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bb396b83834db386832774e6088c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6690abfe614e809656c3fac46c9954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9ba16b120c42cb869a4ae7d28a8577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-13 15:10:39,034] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2026-02-13 15:10:39,035] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2026-02-13 15:10:39,036] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2026-02-13 15:10:39,039] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n",
      "[2026-02-13 15:10:39,245] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2026-02-13 15:10:39,247] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2026-02-13 15:10:39,248] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2026-02-13 15:10:39,280] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2026-02-13 15:10:39,281] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2026-02-13 15:10:39,282] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2026-02-13 15:10:39,283] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
      "[2026-02-13 15:10:39,283] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
      "[2026-02-13 15:10:39,284] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2026-02-13 15:10:39,285] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.75 GiB. GPU 0 has a total capacity of 23.56 GiB of which 4.90 GiB is free. Including non-PyTorch memory, this process has 18.25 GiB memory in use. Of the allocated memory 11.50 GiB is allocated by PyTorch, and 6.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m policy_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Initialize DeepSpeed engines\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m policy_model, *_ = \u001b[43mdeepspeed\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeepspeed_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m reference_model, *_ = deepspeed.initialize(\n\u001b[32m     24\u001b[39m     model=reference_model,\n\u001b[32m     25\u001b[39m     config=ref_deepspeed_config,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m reference_model.module.cpu()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rl-1/lib/python3.12/site-packages/deepspeed/__init__.py:193\u001b[39m, in \u001b[36minitialize\u001b[39m\u001b[34m(args, model, optimizer, model_parameters, training_data, lr_scheduler, distributed_port, mpu, dist_init_required, collate_fn, config, mesh_param, config_params)\u001b[39m\n\u001b[32m    181\u001b[39m         engine = DeepSpeedHybridEngine(args=args,\n\u001b[32m    182\u001b[39m                                        model=model,\n\u001b[32m    183\u001b[39m                                        optimizer=optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    190\u001b[39m                                        config=config,\n\u001b[32m    191\u001b[39m                                        config_class=config_class)\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m         engine = \u001b[43mDeepSpeedEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mdist_init_required\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdist_init_required\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmesh_device\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmesh_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mconfig_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mmpu must be None with pipeline parallelism\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rl-1/lib/python3.12/site-packages/deepspeed/runtime/engine.py:317\u001b[39m, in \u001b[36mDeepSpeedEngine.__init__\u001b[39m\u001b[34m(self, args, model, optimizer, model_parameters, training_data, lr_scheduler, mpu, dist_init_required, collate_fn, config, config_class, mesh_device, dont_change_device)\u001b[39m\n\u001b[32m    314\u001b[39m     model_parameters = \u001b[38;5;28mlist\u001b[39m(model_parameters)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_optimizer:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_configure_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28mself\u001b[39m._configure_lr_scheduler()\n\u001b[32m    319\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_progress(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rl-1/lib/python3.12/site-packages/deepspeed/runtime/engine.py:1385\u001b[39m, in \u001b[36mDeepSpeedEngine._configure_optimizer\u001b[39m\u001b[34m(self, client_optimizer, model_parameters)\u001b[39m\n\u001b[32m   1382\u001b[39m optimizer_wrapper = \u001b[38;5;28mself\u001b[39m._do_optimizer_sanity_check(basic_optimizer)\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer_wrapper == ZERO_OPTIMIZATION:\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_configure_zero_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m optimizer_wrapper == AMP:\n\u001b[32m   1387\u001b[39m     amp_params = \u001b[38;5;28mself\u001b[39m.amp_params()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rl-1/lib/python3.12/site-packages/deepspeed/runtime/engine.py:1643\u001b[39m, in \u001b[36mDeepSpeedEngine._configure_zero_optimizer\u001b[39m\u001b[34m(self, optimizer)\u001b[39m\n\u001b[32m   1641\u001b[39m             logger.warning(\u001b[33m\"\u001b[39m\u001b[33mPipeline parallelism does not support overlapped communication, will be disabled.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1642\u001b[39m             overlap_comm = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1643\u001b[39m     optimizer = \u001b[43mDeepSpeedZeroOptimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1644\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatic_loss_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_loss_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdynamic_loss_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_loss_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdynamic_loss_scale_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_clipping\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontiguous_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontiguous_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduce_bucket_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_reduce_bucket_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_multi_rank_bucket_allreduce\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_multi_rank_bucket_allreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallgather_bucket_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_allgather_bucket_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdp_process_group\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_data_parallel_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpert_parallel_group\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexpert_parallel_group\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhas_moe_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpert_data_parallel_group\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexpert_data_parallel_group\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhas_moe_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduce_scatter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_reduce_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverlap_comm\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverlap_comm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1660\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_optimizer_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_offload_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmpu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpostscale_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpostscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_predivide_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_predivide_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_unused_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_ignore_unused_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartition_grads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzero_stage\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mZeroStageEnum\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mround_robin_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[43mround_robin_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_moe_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhas_moe_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfp16_master_weights_and_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp16_master_weights_and_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_accumulation_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcommunication_data_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommunication_data_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m        \u001b[49m\u001b[43melastic_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_elastic_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m zero_stage == ZeroStageEnum.weights:\n\u001b[32m   1675\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_moe_layers, \u001b[33m\"\u001b[39m\u001b[33mMoE not supported with Stage 3\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rl-1/lib/python3.12/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:401\u001b[39m, in \u001b[36mDeepSpeedZeroOptimizer.__init__\u001b[39m\u001b[34m(self, init_optimizer, param_names, timers, static_loss_scale, dynamic_loss_scale, dynamic_loss_args, verbose, contiguous_gradients, reduce_bucket_size, use_multi_rank_bucket_allreduce, allgather_bucket_size, dp_process_group, expert_parallel_group, expert_data_parallel_group, reduce_scatter, overlap_comm, offload_optimizer_config, mpu, clip_grad, gradient_accumulation_dtype, communication_data_type, postscale_gradients, gradient_predivide_factor, gradient_accumulation_steps, ignore_unused_parameters, partition_grads, round_robin_gradients, has_moe_layers, fp16_master_weights_and_gradients, elastic_checkpoint)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# A partition of the fp32 master weights that will be updated by this process.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Note that the params in single_partition_of_fp32_groups is cloned and detached\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[38;5;66;03m# from the origin params of the model.\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp16_master_weights_and_gradients:\n\u001b[32m    400\u001b[39m     weights_partition = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_partitioned_bit16_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpartition_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.float().detach()\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    403\u001b[39m     weights_partition = \u001b[38;5;28mself\u001b[39m.parallel_partitioned_bit16_groups[i][partition_id].to(\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m.device).clone().half().detach()\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 5.75 GiB. GPU 0 has a total capacity of 23.56 GiB of which 4.90 GiB is free. Including non-PyTorch memory, this process has 18.25 GiB memory in use. Of the allocated memory 11.50 GiB is allocated by PyTorch, and 6.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Initialize main and reference models\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=0,\n",
    ")\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=0,\n",
    ")\n",
    "policy_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "\n",
    "# Initialize DeepSpeed engines\n",
    "policy_model, *_ = deepspeed.initialize(\n",
    "    model=policy_model,\n",
    "    config=deepspeed_config,\n",
    "    model_parameters=policy_model.parameters(),\n",
    ")\n",
    "reference_model, *_ = deepspeed.initialize(\n",
    "    model=reference_model,\n",
    "    config=ref_deepspeed_config,\n",
    ")\n",
    "\n",
    "reference_model.module.cpu()\n",
    "\n",
    "############################################\n",
    "# Initialize vLLM (Inference) engine\n",
    "############################################\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    skip_tokenizer_init=False,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    enable_prefix_caching=True,\n",
    "    swap_space=1,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2048,\n",
    "    enable_sleep_mode=True,\n",
    ")\n",
    "\n",
    "# Wandb for logging\n",
    "wandb.init(\n",
    "    project=\"r1-aha-moment\",\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_iterations\": NUM_ITERATIONS,\n",
    "        \"episodes_per_iteration\": EPISODES_PER_ITERATION,\n",
    "        \"rollouts_per_episode\": GENERATIONS_PER_SAMPLE,\n",
    "        \"kl_coefficient\": KL_COEFFICIENT,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "begin_iter = 0\n",
    "ckpt_path, ckpt_iter = find_last_checkpoint(EXP_DIR)\n",
    "if ckpt_path is not None:\n",
    "    print(f\"Resuming from checkpoint {ckpt_path} at iteration {ckpt_iter}\")\n",
    "    out = policy_model.load_checkpoint(ckpt_path / \"deepspeed\")\n",
    "    if out is None:\n",
    "        raise RuntimeError(f\"Failed to load checkpoint {ckpt_path}\")\n",
    "    begin_iter = ckpt_iter + 1\n",
    "    load_model_into_vllm(policy_model, inference_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba132ef-bad6-4314-8f66-b97ab9a9a18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c1ca4-3c0a-4c2c-9d30-6b0650e72f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b1db5-bf88-494e-b80f-5bdcfb6f35c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d86e03-cb2e-4dbd-9423-56c912e1b140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20a34f-6abf-4f92-833a-3e1b01cb37f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043716f9-7ec8-4050-b559-970b13701c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

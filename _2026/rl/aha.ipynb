{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nanoAhaMoment: Single File R1-Zero Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Installation Requirements\n\nInstall the required packages:\n\n```txt\n# Core dependencies\ntorch==2.9.1\n\n# Deep learning frameworks\ntransformers>=4.57.0\ndatasets>=4.5.0\ndeepspeed>=0.18.0\naccelerate\n\n# Flash Attention for A100\nflash-attn\n\n# vLLM for inference\nvllm>=0.14.1\n\n# Training and logging\nwandb\ntqdm\n\n# Standard scientific computing\nnumpy\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install torch==2.9.1\n!pip install transformers datasets deepspeed accelerate vllm wandb tqdm numpy\n!pip install flash-attn --no-build-isolation"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SCRATCH = Path.cwd() / \"scratch\"\n",
    "os.environ[\"HF_HOME\"] = str(SCRATCH / \"hf_home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from __future__ import annotations\n\nimport gc\nimport re\nimport time\nimport sys\nfrom typing import Any, Dict, List, Tuple, Union, TYPE_CHECKING\n\nimport torch\n\nimport deepspeed\nimport numpy as np\nfrom datasets import load_dataset\nfrom deepspeed import DeepSpeedEngine\nfrom tqdm import trange\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Direct imports to avoid vLLM lazy import issues\nfrom vllm.entrypoints.llm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel\n\nimport wandb\nfrom utils import (\n    compute_token_log_probs,\n    dump_episodes,\n    evaluate_on_test_set,\n    find_free_port,\n    find_last_checkpoint,\n    prepare_model_inputs,\n    load_model_into_vllm\n)\n\n# Needed to stop DeepSpeed from complaining\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = str(find_free_port())\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"LOCAL_RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model configuration\nMODEL_NAME = \"Qwen/Qwen2.5-3B\"\nMODEL_CHAT_NAME = MODEL_NAME + \"-Instruct\"\n\n# Dataset configuration\nDATASET_NAME = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n\n# Total number of training iterations\nNUM_ITERATIONS = 1000\n# Number of episodes to collect per iteration for training\nEPISODES_PER_ITERATION = 64\n# Number of responses to generate for each input prompt (i.e. group size in GRPO)\nGENERATIONS_PER_SAMPLE = 4\n# Controls how much the policy can deviate from the reference model\nKL_COEFFICIENT = 0.001\n\n# Training hyperparameters\n# Batch size for each GPU device during training\nPER_DEVICE_BATCH_SIZE = 4\n# Learning rate for model updates\nLEARNING_RATE = 1e-6\n\n# Sampling parameters\n# Maximum number of tokens to generate in each response\nMAX_RESPONSE_TOKENS = 1024\n# Controls randomness in generation (higher = more random)\nTEMPERATURE = 1.0\n# Nucleus sampling parameter (1.0 = disabled)\nTOP_P = 1.0\n# Top-k sampling parameter (-1 = disabled)\nTOP_K = -1  # no top k\n\n# DeepSpeed configuration\n# DeepSpeed config for the policy model\ndeepspeed_config = {\n    \"bf16\": {\"enabled\": True},\n    \"zero_optimization\": {\"stage\": 2, \"overlap_comm\": False},\n    \"train_batch_size\": EPISODES_PER_ITERATION,\n    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n    \"gradient_clipping\": 1.0,\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": LEARNING_RATE,\n            \"betas\": (0.9, 0.999),\n            \"eps\": 1e-8,\n            \"weight_decay\": 0.0,\n            \"torch_adam\": True,\n        },\n    },\n}\n# DeepSpeed config for the reference model\nref_deepspeed_config = {\n    \"bf16\": {\"enabled\": True},\n    # Note that we don't train the reference model\n    # These are just for compatibility with DeepSpeed.\n    \"train_batch_size\": EPISODES_PER_ITERATION,\n    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n}\n\nRUN_NAME = \"r1-zero\"\nEXP_DIR = SCRATCH / \"deepseek_r1z_hackathon\" / RUN_NAME\nEXP_DIR.mkdir(parents=True, exist_ok=True)\nprint(f\"Logs and Checkpoints will be saved to: {EXP_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the training prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = (\n",
    "    \"You are a helpful assistant. You first think about the reasoning process in the mind \"\n",
    "    \"and then provide the user with the answer.\"\n",
    ")\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Using the numbers {numbers}, create an equation that equals {target}. \"\n",
    "    \"You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. \"\n",
    "    \"Show your work in <think> </think> tags. And return the final equation and answer in \"\n",
    "    \"<answer> </answer> tags, for example <answer>(1 + 2) / (3 * 5)</answer>.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489864, 500)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process dataset\n",
    "def preprocess_example(example: Dict[str, Any]):\n",
    "    numbers: List[int] = example[\"nums\"]\n",
    "    target: int = example[\"target\"]\n",
    "\n",
    "    prefix = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(numbers=numbers, target=target)},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prefix, tokenize=True, continue_final_message=True\n",
    "    )\n",
    "    prompt = tokenizer.decode(\n",
    "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"input_ids\": input_ids}\n",
    "\n",
    "# Note that the base model and \"instruct\" model have different eos token. \n",
    "# Here we make sure to use the correct one.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHAT_NAME)\n",
    "EOS_TOKEN_ID = AutoTokenizer.from_pretrained(MODEL_NAME).eos_token_id\n",
    "EOS_TOKEN = tokenizer.convert_ids_to_tokens(EOS_TOKEN_ID)\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "dataset = dataset.map(preprocess_example, num_proc=6)\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = dataset.train_test_split(test_size=500, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completion: str) -> float:\n",
    "    \"\"\"Check if output follows <think>...</think>\\n<answer>...</answer> format.\"\"\"\n",
    "    allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "\n",
    "    try:\n",
    "        completion = \"<think>\" + completion\n",
    "        if completion.endswith(EOS_TOKEN):\n",
    "            completion = completion[:-len(EOS_TOKEN)]\n",
    "\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            return 0.0\n",
    "        else:\n",
    "            answer_content = match.group(2).strip()\n",
    "            if not re.match(allowed_pattern, answer_content):\n",
    "                return 0.5\n",
    "            else:\n",
    "                return 1.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def equation_reward_func(completion: str, nums: List[int], target: int) -> float:\n",
    "    \"\"\"Check if the equation in <answer> tags evaluates to the target using all numbers exactly once.\"\"\"\n",
    "    try:\n",
    "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "        if match is None:\n",
    "            return 0.0\n",
    "\n",
    "        equation = match.group(1).strip()\n",
    "        used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n",
    "\n",
    "        if sorted(used_numbers) != sorted(nums):\n",
    "            return 0.0\n",
    "\n",
    "        allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "        if not re.match(allowed_pattern, equation):\n",
    "            return 0.0\n",
    "\n",
    "        result = eval(equation, {\"__builtins__\": None}, {})\n",
    "        if abs(float(result) - float(target)) < 1e-5:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "def compute_reward(completion: str, sample: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:\n",
    "    nums = sample[\"nums\"]\n",
    "    target = sample[\"target\"]\n",
    "\n",
    "    format_reward = format_reward_func(completion)\n",
    "    equation_reward = equation_reward_func(completion=completion, nums=nums, target=target)\n",
    "\n",
    "    reward = format_reward + equation_reward\n",
    "    metrics = {\n",
    "        \"format_reward\": format_reward,\n",
    "        \"equation_reward\": equation_reward,\n",
    "    }   \n",
    "\n",
    "    return reward, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_episodes(\n",
    "    samples: List[Dict[str, Any]],\n",
    "    all_generations: List[List[int]],\n",
    "    all_finish_reasons: List[str],\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"Process generations into training episodes with GRPO advantages.\"\"\"\n",
    "    assert len(all_generations) == len(all_finish_reasons)\n",
    "    assert len(all_generations) == len(samples) * GENERATIONS_PER_SAMPLE\n",
    "\n",
    "    groups = [\n",
    "        list(range(i, i + GENERATIONS_PER_SAMPLE))\n",
    "        for i in range(0, len(all_generations), GENERATIONS_PER_SAMPLE)\n",
    "    ]\n",
    "\n",
    "    all_query_token_ids, all_responses_token_ids, all_advantages = [], [], []\n",
    "\n",
    "    stats = {\n",
    "        \"response_lengths\": [],\n",
    "        \"rewards\": [],\n",
    "        \"non_stop_rate\": [],\n",
    "    }\n",
    "\n",
    "    for sample, group_indices in zip(samples, groups):\n",
    "        finish_reasons = [all_finish_reasons[i] for i in group_indices]\n",
    "        response_token_ids = [all_generations[i] for i in group_indices]\n",
    "        responses = tokenizer.batch_decode(response_token_ids, skip_special_tokens=False)\n",
    "\n",
    "        rewards_and_metrics = [compute_reward(resp, sample) for resp in responses]\n",
    "        rewards, reward_metrics = zip(*rewards_and_metrics)\n",
    "\n",
    "        # GRPO advantage: normalize rewards within the group\n",
    "        rewards = np.array(rewards)\n",
    "        response_advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4)\n",
    "        \n",
    "        # Assign same advantage to all tokens in each response\n",
    "        advantages = [\n",
    "            [resp_adv] * len(resp) \n",
    "            for resp_adv, resp in zip(response_advantages, response_token_ids)\n",
    "        ]\n",
    "\n",
    "        all_query_token_ids.extend([sample[\"input_ids\"]] * GENERATIONS_PER_SAMPLE)\n",
    "        all_responses_token_ids.extend(response_token_ids)\n",
    "        all_advantages.extend(advantages)\n",
    "\n",
    "        stats[\"rewards\"].extend(rewards)\n",
    "        stats[\"non_stop_rate\"].extend([fr != \"stop\" for fr in finish_reasons])\n",
    "        stats[\"response_lengths\"].extend([len(ids) for ids in response_token_ids])\n",
    "        for rm in reward_metrics:\n",
    "            for k, v in rm.items():\n",
    "                stats.setdefault(f\"reward_metrics/{k}\", []).append(v)\n",
    "\n",
    "    episodes = {\n",
    "        \"all_query_token_ids\": all_query_token_ids,\n",
    "        \"all_response_token_ids\": all_responses_token_ids,\n",
    "        \"all_advantages\": all_advantages,\n",
    "    }\n",
    "\n",
    "    return episodes, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_pg_loss(\n    policy_model: Union[DeepSpeedEngine, PreTrainedModel],\n    reference_model: Union[DeepSpeedEngine, PreTrainedModel],\n    batch: Dict[str, torch.Tensor],\n    total_response_len: int,\n) -> Tuple[torch.Tensor, Dict[str, float]]:\n    \"\"\"Compute policy gradient loss with KL penalty between policy and reference models.\"\"\"\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n    labels = batch[\"labels\"]\n    advantages = batch[\"advantages\"]\n\n    model_inputs = {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels,\n        \"labels_mask\": batch[\"labels_mask\"],\n    }\n\n    labels_mask = (labels[..., 1:] != -100).float()\n\n    with torch.no_grad():\n        ref_logps = compute_token_log_probs(reference_model, model_inputs, TEMPERATURE)\n\n    logps = compute_token_log_probs(policy_model, model_inputs, TEMPERATURE)\n\n    # KL penalty (k3 estimator)\n    kl_penalty = torch.exp(ref_logps - logps) - (ref_logps - logps) - 1\n    kl_penalty = kl_penalty * labels_mask\n\n    entropy = -logps.sum() / labels_mask.sum()\n\n    # Policy gradient loss: -log_prob * advantage\n    policy_loss = -logps * advantages[..., 1:]\n    policy_loss = policy_loss * labels_mask\n\n    loss = (policy_loss + KL_COEFFICIENT * kl_penalty).sum() / total_response_len\n\n    metrics = {\n        \"policy_loss\": policy_loss.sum().item() / total_response_len,\n        \"kl_penalty\": kl_penalty.sum().item() / total_response_len,\n        \"entropy\": entropy.item() / total_response_len,\n    }\n\n    return loss, metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize main and reference models\npolicy_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n    device_map=0,\n)\nreference_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n    device_map=0,\n)\npolicy_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n\n\n# Initialize DeepSpeed engines\npolicy_model, *_ = deepspeed.initialize(\n    model=policy_model,\n    config=deepspeed_config,\n    model_parameters=policy_model.parameters(),\n)\nreference_model, *_ = deepspeed.initialize(\n    model=reference_model,\n    config=ref_deepspeed_config,\n)\n\nreference_model.module.cpu()\n\n############################################\n# Initialize vLLM (Inference) engine\n############################################\n\ninference_engine = LLM(\n    model=MODEL_NAME,\n    skip_tokenizer_init=False,\n    gpu_memory_utilization=0.2,\n    enable_prefix_caching=True,\n    swap_space=1,\n    scheduling_policy=\"fcfs\",\n    dtype=torch.bfloat16,\n    max_model_len=2048,\n    enable_sleep_mode=True,\n)\n\n# Wandb for logging\nwandb.init(\n    project=\"r1-aha-moment\",\n    name=RUN_NAME,\n    config={\n        \"model_name\": MODEL_NAME,\n        \"learning_rate\": LEARNING_RATE,\n        \"num_iterations\": NUM_ITERATIONS,\n        \"episodes_per_iteration\": EPISODES_PER_ITERATION,\n        \"rollouts_per_episode\": GENERATIONS_PER_SAMPLE,\n        \"kl_coefficient\": KL_COEFFICIENT,\n        \"temperature\": TEMPERATURE,\n    },\n)\n\n# Load checkpoint if it exists\nbegin_iter = 0\nckpt_path, ckpt_iter = find_last_checkpoint(EXP_DIR)\nif ckpt_path is not None:\n    print(f\"Resuming from checkpoint {ckpt_path} at iteration {ckpt_iter}\")\n    out = policy_model.load_checkpoint(ckpt_path / \"deepspeed\")\n    if out is None:\n        raise RuntimeError(f\"Failed to load checkpoint {ckpt_path}\")\n    begin_iter = ckpt_iter + 1\n    load_model_into_vllm(policy_model, inference_engine)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in trange(NUM_ITERATIONS):\n",
    "    print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    #########################################################\n",
    "    # Evaluation\n",
    "    #########################################################\n",
    "\n",
    "    eval_stats = None\n",
    "    if iteration % 25 == 0:\n",
    "        print(\"Evaluating on eval set...\")\n",
    "        eval_episodes, eval_stats = evaluate_on_test_set(\n",
    "            inference_engine=inference_engine,\n",
    "            test_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            eos_token=EOS_TOKEN,\n",
    "            eval_sampling_params=SamplingParams(\n",
    "                temperature=0.3,\n",
    "                max_tokens=1024,\n",
    "                n=1,\n",
    "                detokenize=False,\n",
    "                stop_token_ids=[EOS_TOKEN_ID],\n",
    "            ),\n",
    "            reward_func=lambda completion, sample: compute_reward(\n",
    "                completion, sample\n",
    "            ),\n",
    "        )\n",
    "        eval_episode_table = dump_episodes(\n",
    "            episodes=eval_episodes,\n",
    "            episodes_stats=eval_stats,\n",
    "            exp_dir=EXP_DIR,\n",
    "            tokenizer=tokenizer,\n",
    "            iteration=iteration,\n",
    "            is_eval=True,\n",
    "        )\n",
    "        wandb.log({\"eval/episodes\": eval_episode_table, \"iteration\": iteration})\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Generate Episodes\n",
    "    #########################################################\n",
    "\n",
    "    # Sample training batch\n",
    "    num_samples = EPISODES_PER_ITERATION // GENERATIONS_PER_SAMPLE\n",
    "    indices = np.random.choice(\n",
    "        len(train_dataset), size=num_samples, replace=False\n",
    "    )\n",
    "    samples = train_dataset.select(indices)\n",
    "\n",
    "    # Sample responses\n",
    "    outputs = inference_engine.generate(\n",
    "        prompt_token_ids=samples[\"input_ids\"],\n",
    "        sampling_params=SamplingParams(\n",
    "            n=GENERATIONS_PER_SAMPLE,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            max_tokens=MAX_RESPONSE_TOKENS,\n",
    "            detokenize=False,\n",
    "            stop_token_ids=[EOS_TOKEN_ID],\n",
    "        )\n",
    "    )\n",
    "    all_generations = [list(g.token_ids) for out in outputs for g in out.outputs]\n",
    "    all_finish_reasons = [g.finish_reason for out in outputs for g in out.outputs]\n",
    "    inference_engine.sleep(1)\n",
    "\n",
    "    print(f\"Generated {len(all_generations)} responses\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Process responses and calculate rewards\n",
    "    episodes, episodes_stats = create_training_episodes(\n",
    "        samples,\n",
    "        all_generations,\n",
    "        all_finish_reasons,\n",
    "    )\n",
    "    for k, v in episodes_stats.items():\n",
    "        metrics.setdefault(k, []).extend(v)\n",
    "\n",
    "    episode_table = dump_episodes(\n",
    "        episodes=episodes,\n",
    "        episodes_stats=episodes_stats,\n",
    "        exp_dir=EXP_DIR,\n",
    "        tokenizer=tokenizer,\n",
    "        iteration=iteration,\n",
    "    )\n",
    "\n",
    "    #########################################################\n",
    "    # Training\n",
    "    #########################################################\n",
    "\n",
    "    # Prepare training batch\n",
    "    model_inputs = prepare_model_inputs(\n",
    "        query_token_ids=episodes[\"all_query_token_ids\"],\n",
    "        response_token_ids=episodes[\"all_response_token_ids\"],\n",
    "        advantages=episodes[\"all_advantages\"],\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    # Calculate losses and update model\n",
    "    policy_model.train()\n",
    "    reference_model.module.cuda()\n",
    "    reference_model.eval()\n",
    "\n",
    "    total_response_len = (model_inputs[\"labels\"] != -100).sum().item()\n",
    "\n",
    "    for i in trange(0, EPISODES_PER_ITERATION, PER_DEVICE_BATCH_SIZE, desc=\"Gradient Accumulation\"):\n",
    "        batch = {\n",
    "            k: v[i : i + PER_DEVICE_BATCH_SIZE]\n",
    "            for k, v in model_inputs.items()\n",
    "        }\n",
    "\n",
    "        # Compute policy gradient loss\n",
    "        loss, loss_metrics = compute_pg_loss(\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            batch=batch,\n",
    "            total_response_len=total_response_len,\n",
    "        )\n",
    "\n",
    "        # Track metrics\n",
    "        metrics.setdefault(\"loss\", []).append(loss.item())\n",
    "        grad_norm = policy_model.get_global_grad_norm()\n",
    "        if grad_norm is not None:\n",
    "            grad_norm = grad_norm.item()\n",
    "        metrics.setdefault(\"grad_norm\", []).append(grad_norm)\n",
    "        for k, v in loss_metrics.items():\n",
    "            metrics.setdefault(k, []).append(v.item() if isinstance(v, torch.Tensor) else v)\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        policy_model.backward(loss, scale_wrt_gas=False)\n",
    "        \n",
    "        # Free memory\n",
    "        del loss, loss_metrics\n",
    "        if policy_model.is_gradient_accumulation_boundary():\n",
    "            reference_model.module.cpu()\n",
    "\n",
    "        policy_model.step()\n",
    "\n",
    "    #########################################################\n",
    "    # Update inference engine weights\n",
    "    #########################################################\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "    inference_engine.wake_up()\n",
    "    load_model_into_vllm(policy_model, inference_engine)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Log metrics\n",
    "    #########################################################\n",
    "\n",
    "    train_metrics = {\n",
    "        k: np.mean(v) for k, v in metrics.items() if None not in v\n",
    "    }\n",
    "    train_metrics[\"learning_rate\"] = policy_model.get_lr()[0]\n",
    "    logs = {\n",
    "        \"iteration\": iteration,\n",
    "        f\"episodes/iter_{iteration:06d}\": episode_table,\n",
    "        **{f\"train/{k}\": v for k, v in train_metrics.items()},\n",
    "    }\n",
    "    if eval_stats is not None:\n",
    "        eval_metrics = {k: np.mean(v) for k, v in eval_stats.items() if None not in v}\n",
    "        logs.update({f\"eval/{k}\": v for k, v in eval_metrics.items()})\n",
    "    wandb.log(logs)\n",
    "\n",
    "    selected_keys = [\n",
    "        \"train/kl_penalty\",\n",
    "        \"train/rewards\",\n",
    "        \"train/reward_metrics/format_reward\",\n",
    "        \"train/reward_metrics/equation_reward\",\n",
    "        \"eval/rewards\",\n",
    "        \"eval/reward_metrics/format_reward\",\n",
    "        \"eval/reward_metrics/equation_reward\",\n",
    "    ]\n",
    "    selected_metrics = {k: logs[k] for k in selected_keys if k in logs}\n",
    "    print(f\"KEY METRICS: {selected_metrics}\")\n",
    "\n",
    "    if iteration % 50 == 0 and iteration != 0:\n",
    "        policy_model.module.save_pretrained(\n",
    "            str(EXP_DIR / \"checkpoints\" / f\"ckpt_{iteration:06d}\" / \"hf_model\")\n",
    "        )\n",
    "        policy_model.save_checkpoint(\n",
    "            str(EXP_DIR / \"checkpoints\" / f\"ckpt_{iteration:06d}\" / \"deepspeed\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport glob as glob_module\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\n\ndef load_episodes(episode_dir: Path) -> Dict[int, List[Dict]]:\n    \"\"\"Load all saved episode JSON files from a directory, keyed by iteration number.\"\"\"\n    episodes_by_iter = {}\n    for fpath in sorted(episode_dir.glob(\"eps_*.json\")):\n        iteration = int(fpath.stem.split(\"_\")[-1])\n        with open(fpath) as f:\n            episodes_by_iter[iteration] = json.load(f)\n    return episodes_by_iter\n\ntrain_episodes = load_episodes(EXP_DIR / \"episodes\")\neval_episodes = load_episodes(EXP_DIR / \"eval_episodes\")\n\nprint(f\"Loaded train episodes from {len(train_episodes)} iterations\")\nprint(f\"Loaded eval episodes from {len(eval_episodes)} iterations\")\nprint(f\"Train iterations: {sorted(train_episodes.keys())[:5]} ... {sorted(train_episodes.keys())[-5:]}\")\nprint(f\"Eval iterations: {sorted(eval_episodes.keys())[:5]} ... {sorted(eval_episodes.keys())[-5:]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### How did performance increase on the countdown task?\n\nWe track two reward components across training:\n- **Format reward** (0 or 0.5 or 1.0): Does the model produce valid `<think>...</think>\\n<answer>...</answer>` formatting?\n- **Equation reward** (0 or 1.0): Does the equation in the answer actually evaluate to the target using all numbers exactly once?\n\nThe total reward is the sum of both (max 2.0). We plot these over training iterations for both the training rollouts and the held-out eval set.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_episode_metrics(episodes_by_iter: Dict[int, List[Dict]]) -> Dict[str, List]:\n    \"\"\"Compute per-iteration aggregate metrics from saved episodes.\"\"\"\n    iterations = sorted(episodes_by_iter.keys())\n    metrics = {\"iteration\": [], \"mean_reward\": [], \"format_rate\": [], \"equation_rate\": [], \"mean_response_len\": []}\n\n    for it in iterations:\n        eps = episodes_by_iter[it]\n        rewards = [e[\"reward\"] for e in eps]\n        responses = [e[\"response\"] for e in eps]\n\n        # Re-derive format and equation rewards from the saved responses\n        format_rewards = [format_reward_func(r) for r in responses]\n        equation_rewards = []\n        for e in eps:\n            # Extract nums and target from the query text\n            query = e[\"query\"]\n            nums_match = re.search(r\"numbers \\[([^\\]]+)\\]\", query)\n            target_match = re.search(r\"equals (\\d+)\", query)\n            if nums_match and target_match:\n                nums = [int(x.strip()) for x in nums_match.group(1).split(\",\")]\n                target = int(target_match.group(1))\n                equation_rewards.append(equation_reward_func(e[\"response\"], nums, target))\n            else:\n                equation_rewards.append(0.0)\n\n        metrics[\"iteration\"].append(it)\n        metrics[\"mean_reward\"].append(np.mean(rewards))\n        metrics[\"format_rate\"].append(np.mean(format_rewards))\n        metrics[\"equation_rate\"].append(np.mean(equation_rewards))\n        metrics[\"mean_response_len\"].append(np.mean([len(r) for r in responses]))\n\n    return metrics\n\ntrain_metrics = compute_episode_metrics(train_episodes)\neval_metrics = compute_episode_metrics(eval_episodes)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Plot 1: Total reward\naxes[0].plot(train_metrics[\"iteration\"], train_metrics[\"mean_reward\"], alpha=0.4, label=\"Train\")\nif eval_metrics[\"iteration\"]:\n    axes[0].plot(eval_metrics[\"iteration\"], eval_metrics[\"mean_reward\"], \"o-\", label=\"Eval\", markersize=3)\naxes[0].set_xlabel(\"Iteration\")\naxes[0].set_ylabel(\"Mean Reward\")\naxes[0].set_title(\"Total Reward over Training\")\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot 2: Format reward vs Equation reward (eval)\nsource = eval_metrics if eval_metrics[\"iteration\"] else train_metrics\naxes[1].plot(source[\"iteration\"], source[\"format_rate\"], \"o-\", label=\"Format Reward\", markersize=3)\naxes[1].plot(source[\"iteration\"], source[\"equation_rate\"], \"s-\", label=\"Equation Reward\", markersize=3)\naxes[1].set_xlabel(\"Iteration\")\naxes[1].set_ylabel(\"Mean Reward Component\")\naxes[1].set_title(\"Reward Components (Eval)\" if eval_metrics[\"iteration\"] else \"Reward Components (Train)\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Plot 3: Response length\naxes[2].plot(train_metrics[\"iteration\"], train_metrics[\"mean_response_len\"], alpha=0.4, label=\"Train\")\nif eval_metrics[\"iteration\"]:\n    axes[2].plot(eval_metrics[\"iteration\"], eval_metrics[\"mean_response_len\"], \"o-\", label=\"Eval\", markersize=3)\naxes[2].set_xlabel(\"Iteration\")\naxes[2].set_ylabel(\"Mean Response Length (chars)\")\naxes[2].set_title(\"Response Length over Training\")\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print summary table\nif eval_metrics[\"iteration\"]:\n    first_iter, last_iter = eval_metrics[\"iteration\"][0], eval_metrics[\"iteration\"][-1]\n    print(f\"\\n{'Metric':<25} {'Iter ' + str(first_iter):<15} {'Iter ' + str(last_iter):<15} {'Change':<15}\")\n    print(\"-\" * 70)\n    for key in [\"mean_reward\", \"format_rate\", \"equation_rate\"]:\n        v0, v1 = eval_metrics[key][0], eval_metrics[key][-1]\n        print(f\"{key:<25} {v0:<15.4f} {v1:<15.4f} {v1-v0:+.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Do we see things like the \"aha moment\" in the rollouts?\n\nThe \"aha moment\" from DeepSeek-R1 refers to the model spontaneously developing self-verification and correction behaviors during RL training -- without being explicitly taught to do so. We look for rollouts where the model:\n\n- **Re-checks its own work** (\"Wait, let me verify...\", \"Hmm, that doesn't work...\")\n- **Backtracks and tries again** (\"No, that's wrong. Let me try a different approach...\")\n- **Self-verifies the answer** (\"Let me check: 3 + 5 = 8. Yes, that's correct!\")\n\nWe search through training rollouts for these patterns, focusing on high-reward episodes where the model got the right answer after showing signs of self-correction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Patterns that indicate self-correction / \"aha moment\" behaviors\nAHA_PATTERNS = [\n    r\"[Ww]ait\",\n    r\"[Hh]mm\",\n    r\"[Aa]ctually\",\n    r\"[Ll]et me (try|check|verify|reconsider|re-?evaluate|think)\",\n    r\"[Nn]o,?\\s*(that|this)('s|\\s+is)?\\s*(not|wrong|incorrect)\",\n    r\"[Tt]hat('s|\\s+is)?\\s*(not|wrong|incorrect)\",\n    r\"[Dd]oesn'?t\\s*(work|equal|add up|give)\",\n    r\"[Ll]et me re\",\n    r\"[Ii] (made a mistake|was wrong|need to)\",\n    r\"[Oo]n second thought\",\n    r\"[Bb]ut wait\",\n    r\"[Ss]o the answer\",\n]\n\ndef find_aha_episodes(episodes_by_iter: Dict[int, List[Dict]], min_reward: float = 1.5) -> List[Dict]:\n    \"\"\"Find episodes that show self-correction behavior AND got a high reward.\"\"\"\n    aha_episodes = []\n    for iteration in sorted(episodes_by_iter.keys()):\n        for ep in episodes_by_iter[iteration]:\n            if ep[\"reward\"] < min_reward:\n                continue\n            response = ep[\"response\"]\n            matched_patterns = []\n            for pattern in AHA_PATTERNS:\n                if re.search(pattern, response):\n                    matched_patterns.append(pattern)\n            if matched_patterns:\n                aha_episodes.append({\n                    \"iteration\": iteration,\n                    \"query\": ep[\"query\"],\n                    \"response\": response,\n                    \"reward\": ep[\"reward\"],\n                    \"patterns\": matched_patterns,\n                    \"num_patterns\": len(matched_patterns),\n                })\n    return aha_episodes\n\naha_results = find_aha_episodes(train_episodes)\nprint(f\"Found {len(aha_results)} high-reward episodes with self-correction patterns\\n\")\n\n# Show when these patterns start appearing\nif aha_results:\n    aha_by_iter = {}\n    for ep in aha_results:\n        aha_by_iter.setdefault(ep[\"iteration\"], []).append(ep)\n\n    print(\"Iterations with aha-moment episodes (count):\")\n    for it in sorted(aha_by_iter.keys())[:20]:\n        print(f\"  Iter {it:>5d}: {len(aha_by_iter[it])} episodes\")\n    if len(aha_by_iter) > 20:\n        print(f\"  ... and {len(aha_by_iter) - 20} more iterations\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display the best \"aha moment\" examples (most self-correction patterns matched)\ndef display_rollout(ep: Dict, title: str = \"\"):\n    \"\"\"Render a single rollout with syntax highlighting for think/answer tags.\"\"\"\n    response = ep[\"response\"]\n\n    # Highlight self-correction phrases\n    highlighted = response\n    for pattern in AHA_PATTERNS:\n        highlighted = re.sub(\n            f\"({pattern})\",\n            r'<span style=\"background-color: #fff3cd; font-weight: bold;\">\\1</span>',\n            highlighted,\n        )\n    # Highlight tags\n    highlighted = highlighted.replace(\"&lt;think&gt;\", \"<b>&lt;think&gt;</b>\")\n    highlighted = highlighted.replace(\"&lt;/think&gt;\", \"<b>&lt;/think&gt;</b>\")\n    highlighted = highlighted.replace(\"&lt;answer&gt;\", \"<b>&lt;answer&gt;</b>\")\n    highlighted = highlighted.replace(\"&lt;/answer&gt;\", \"<b>&lt;/answer&gt;</b>\")\n\n    html = f\"\"\"\n    <div style=\"border: 1px solid #ddd; border-radius: 8px; padding: 16px; margin: 12px 0; background: #fafafa;\">\n        <div style=\"font-weight: bold; font-size: 14px; color: #333; margin-bottom: 8px;\">\n            {title} | Reward: {ep['reward']} | Patterns: {ep.get('num_patterns', '?')}\n        </div>\n        <div style=\"font-family: monospace; white-space: pre-wrap; font-size: 12px; line-height: 1.5; color: #555;\">\n{highlighted}\n        </div>\n    </div>\n    \"\"\"\n    display(HTML(html))\n\nif aha_results:\n    # Sort by number of matched patterns (most \"aha-like\" first)\n    best_aha = sorted(aha_results, key=lambda x: x[\"num_patterns\"], reverse=True)\n\n    print(\"Top 3 'aha moment' examples (most self-correction patterns):\\n\")\n    for i, ep in enumerate(best_aha[:3]):\n        display_rollout(ep, title=f\"Example {i+1} (Iteration {ep['iteration']})\")\nelse:\n    print(\"No aha-moment episodes found. This may be expected if training hasn't run yet.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### How do rollouts (model responses) compare from the beginning to the end of training?\n\nWe compare model responses from early vs. late training iterations side-by-side. This shows how the model evolves from producing unstructured or incorrect outputs to generating well-formatted, correct solutions with reasoning.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def display_rollout_simple(response: str, reward: float, title: str = \"\"):\n    \"\"\"Render a single rollout without pattern highlighting.\"\"\"\n    html = f\"\"\"\n    <div style=\"border: 1px solid #ddd; border-radius: 8px; padding: 16px; margin: 8px 0; background: #fafafa;\">\n        <div style=\"font-weight: bold; font-size: 13px; color: #333; margin-bottom: 8px;\">\n            {title} | Reward: {reward}\n        </div>\n        <div style=\"font-family: monospace; white-space: pre-wrap; font-size: 12px; line-height: 1.5; color: #555;\">\n{response}\n        </div>\n    </div>\n    \"\"\"\n    display(HTML(html))\n\n# Use eval episodes if available (cleaner comparison), fall back to train\ncomparison_source = eval_episodes if eval_episodes else train_episodes\nsorted_iters = sorted(comparison_source.keys())\n\nif len(sorted_iters) >= 2:\n    early_iter = sorted_iters[0]\n    late_iter = sorted_iters[-1]\n    early_eps = comparison_source[early_iter]\n    late_eps = comparison_source[late_iter]\n\n    NUM_EXAMPLES = 3\n\n    for i in range(min(NUM_EXAMPLES, len(early_eps), len(late_eps))):\n        display(HTML(f\"<h4 style='margin-top: 24px;'>Comparison {i+1}</h4>\"))\n        display_rollout_simple(\n            early_eps[i][\"response\"], early_eps[i][\"reward\"],\n            title=f\"EARLY (Iteration {early_iter})\"\n        )\n        display_rollout_simple(\n            late_eps[i][\"response\"], late_eps[i][\"reward\"],\n            title=f\"LATE (Iteration {late_iter})\"\n        )\n\n    # Summary statistics\n    early_rewards = [e[\"reward\"] for e in early_eps]\n    late_rewards = [e[\"reward\"] for e in late_eps]\n    early_lens = [len(e[\"response\"]) for e in early_eps]\n    late_lens = [len(e[\"response\"]) for e in late_eps]\n\n    print(f\"\\n{'='*60}\")\n    print(f\"{'Statistic':<30} {'Iter ' + str(early_iter):<15} {'Iter ' + str(late_iter):<15}\")\n    print(f\"{'='*60}\")\n    print(f\"{'Mean reward':<30} {np.mean(early_rewards):<15.3f} {np.mean(late_rewards):<15.3f}\")\n    print(f\"{'Reward > 0 (%)':<30} {100*np.mean([r > 0 for r in early_rewards]):<15.1f} {100*np.mean([r > 0 for r in late_rewards]):<15.1f}\")\n    print(f\"{'Perfect reward (2.0) (%)':<30} {100*np.mean([r == 2.0 for r in early_rewards]):<15.1f} {100*np.mean([r == 2.0 for r in late_rewards]):<15.1f}\")\n    print(f\"{'Mean response length (chars)':<30} {np.mean(early_lens):<15.0f} {np.mean(late_lens):<15.0f}\")\nelse:\n    print(\"Not enough iterations to compare. Need at least 2 saved checkpoints.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Tracing the GRPO policy gradient for a single example\n\nTo understand how GRPO works concretely, let's walk through the full computation for a single training prompt. We'll:\n\n1. **Start with one prompt** and its group of `GENERATIONS_PER_SAMPLE` (4) responses\n2. **Score each response** with the reward function\n3. **Compute GRPO advantages** by normalizing rewards within the group (subtract mean, divide by std)\n4. **Show how the advantage maps to the policy gradient**: responses with above-average reward get positive advantage (reinforced), below-average get negative advantage (discouraged)\n\nThis is the core mechanism: GRPO doesn't need a learned value function -- it just compares responses within a group to decide which to reinforce.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Pick a training iteration that has some variance in rewards (not all 0s or all 2s)\ndef find_good_example_group(episodes_by_iter: Dict[int, List[Dict]]) -> Tuple[int, int]:\n    \"\"\"Find an iteration and starting index where a group of 4 has mixed rewards.\"\"\"\n    for iteration in sorted(episodes_by_iter.keys()):\n        eps = episodes_by_iter[iteration]\n        # Episodes are stored in groups of GENERATIONS_PER_SAMPLE\n        for group_start in range(0, len(eps) - GENERATIONS_PER_SAMPLE + 1, GENERATIONS_PER_SAMPLE):\n            group = eps[group_start:group_start + GENERATIONS_PER_SAMPLE]\n            rewards = [e[\"reward\"] for e in group]\n            # Want mixed rewards: at least one success and one failure\n            if min(rewards) < max(rewards) and max(rewards) >= 1.5:\n                return iteration, group_start\n    # Fallback: just use the first group from the first iteration\n    first_iter = sorted(episodes_by_iter.keys())[0]\n    return first_iter, 0\n\nif train_episodes:\n    example_iter, example_start = find_good_example_group(train_episodes)\n    group = train_episodes[example_iter][example_start:example_start + GENERATIONS_PER_SAMPLE]\n\n    print(f\"Example from iteration {example_iter}, episodes {example_start}-{example_start + GENERATIONS_PER_SAMPLE - 1}\")\n    print(f\"Query (shared across all {GENERATIONS_PER_SAMPLE} responses):\")\n    print(f\"  {group[0]['query'][:200]}...\")\n    print()\n\n    ##############################\n    # Step 1: Show the raw rewards\n    ##############################\n    print(\"=\" * 70)\n    print(\"STEP 1: Raw rewards for each response in the group\")\n    print(\"=\" * 70)\n    rewards_raw = []\n    for i, ep in enumerate(group):\n        r = ep[\"reward\"]\n        rewards_raw.append(r)\n        # Truncate response for display\n        resp_preview = ep[\"response\"][:150].replace(\"\\n\", \" \")\n        print(f\"  Response {i+1}: reward = {r:.1f}  |  '{resp_preview}...'\")\n\n    ##############################\n    # Step 2: GRPO advantage computation\n    ##############################\n    print(f\"\\n{'=' * 70}\")\n    print(\"STEP 2: GRPO advantage computation\")\n    print(\"=\" * 70)\n    rewards = np.array(rewards_raw)\n    mean_r = rewards.mean()\n    std_r = rewards.std()\n    advantages = (rewards - mean_r) / (std_r + 1e-4)\n\n    print(f\"  Group rewards:     {rewards}\")\n    print(f\"  Mean reward:       {mean_r:.4f}\")\n    print(f\"  Std reward:        {std_r:.4f}\")\n    print(f\"  Advantages:        {advantages}\")\n    print()\n    for i in range(len(group)):\n        direction = \"REINFORCE (increase probability)\" if advantages[i] > 0 else \"DISCOURAGE (decrease probability)\" if advantages[i] < 0 else \"NEUTRAL (no gradient signal)\"\n        print(f\"  Response {i+1}: advantage = {advantages[i]:+.4f} --> {direction}\")\n\n    ##############################\n    # Step 3: How this becomes a gradient\n    ##############################\n    print(f\"\\n{'=' * 70}\")\n    print(\"STEP 3: Policy gradient for each response\")\n    print(\"=\" * 70)\n    print()\n    print(\"For each token t in response i, the policy gradient contribution is:\")\n    print()\n    print(\"  grad_contribution(t) = -advantage_i * d/d(theta) log p(t | context)\")\n    print()\n    print(\"Since advantage is CONSTANT across all tokens in a response (GRPO),\")\n    print(\"the entire response is reinforced or discouraged uniformly:\")\n    print()\n    for i in range(len(group)):\n        n_tokens = len(group[i][\"response\"].split())  # rough word count\n        if advantages[i] > 0:\n            print(f\"  Response {i+1} ({n_tokens:>3d} words): adv={advantages[i]:+.4f} => every token gets pushed UP in probability\")\n        elif advantages[i] < 0:\n            print(f\"  Response {i+1} ({n_tokens:>3d} words): adv={advantages[i]:+.4f} => every token gets pushed DOWN in probability\")\n        else:\n            print(f\"  Response {i+1} ({n_tokens:>3d} words): adv={advantages[i]:+.4f} => no gradient signal\")\n\n    print(f\"\\nThe KL penalty (coefficient={KL_COEFFICIENT}) then regularizes\")\n    print(\"the update to prevent the policy from drifting too far from the reference model.\")\nelse:\n    print(\"No training episodes available. Run training first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
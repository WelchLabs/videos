{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nanoAhaMoment: Single File R1-Zero Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SCRATCH = Path.home() / \"scratch\"\n",
    "os.environ[\"HF_HOME\"] = str(SCRATCH / \"hf_home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0216 10:32:14.755000 63473 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0216 10:32:14.755000 63473 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-16 10:32:18 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "from typing import Any, Dict, List, Tuple, Union, TYPE_CHECKING\n",
    "\n",
    "# Workaround for torchvision circular import issue\n",
    "# Import torch and torchvision first to ensure proper initialization\n",
    "import torch\n",
    "import torchvision  # This needs to be imported early to avoid circular import issues\n",
    "\n",
    "import deepspeed\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from deepspeed import DeepSpeedEngine\n",
    "from tqdm import trange\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Direct imports to avoid vLLM lazy import issues\n",
    "from vllm.entrypoints.llm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from transformers import PreTrainedModel\n",
    "\n",
    "import wandb\n",
    "from utils import (\n",
    "    compute_token_log_probs,\n",
    "    dump_episodes,\n",
    "    evaluate_on_test_set,\n",
    "    find_free_port,\n",
    "    find_last_checkpoint,\n",
    "    prepare_model_inputs,\n",
    "    load_model_into_vllm\n",
    ")\n",
    "\n",
    "# Needed to stop DeepSpeed from complaining\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs and Checkpoints will be saved to: /Users/pranav/scratch/deepseek_r1z_hackathon/r1-zero\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "MODEL_CHAT_NAME = MODEL_NAME + \"-Instruct\"\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "\n",
    "# Total number of training iterations\n",
    "NUM_ITERATIONS = 1000\n",
    "# Number of episodes to collect per iteration for training\n",
    "EPISODES_PER_ITERATION = 64\n",
    "# Number of responses to generate for each input prompt (i.e. group size in GRPO)\n",
    "GENERATIONS_PER_SAMPLE = 4\n",
    "# Controls how much the policy can deviate from the reference model\n",
    "KL_COEFFICIENT = 0.001\n",
    "\n",
    "# Training hyperparameters\n",
    "# Batch size for each GPU device during training\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "# Learning rate for model updates\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "# Sampling parameters\n",
    "# Maximum number of tokens to generate in each response\n",
    "MAX_RESPONSE_TOKENS = 1024\n",
    "# Controls randomness in generation (higher = more random)\n",
    "TEMPERATURE = 1.0\n",
    "# Nucleus sampling parameter (1.0 = disabled)\n",
    "TOP_P = 1.0\n",
    "# Top-k sampling parameter (-1 = disabled)\n",
    "TOP_K = -1  # no top k\n",
    "\n",
    "# DeepSpeed configuration\n",
    "# DeepSpeed config for the policy model\n",
    "deepspeed_config = {\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\"stage\": 2, \"overlap_comm\": False},\n",
    "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
    "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": LEARNING_RATE,\n",
    "            \"betas\": (0.9, 0.999),\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"torch_adam\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "# DeepSpeed config for the reference model\n",
    "ref_deepspeed_config = {\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    # Note that we don't train the reference model\n",
    "    # These are just for compatibility with DeepSpeed.\n",
    "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
    "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
    "}\n",
    "\n",
    "RUN_NAME = \"r1-zero\"\n",
    "EXP_DIR = SCRATCH / \"deepseek_r1z_hackathon\" / RUN_NAME\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Logs and Checkpoints will be saved to: {EXP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the training prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = (\n",
    "    \"You are a helpful assistant. You first think about the reasoning process in the mind \"\n",
    "    \"and then provide the user with the answer.\"\n",
    ")\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Using the numbers {numbers}, create an equation that equals {target}. \"\n",
    "    \"You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. \"\n",
    "    \"Show your work in <think> </think> tags. And return the final equation and answer in \"\n",
    "    \"<answer> </answer> tags, for example <answer>(1 + 2) / (3 * 5)</answer>.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961a4d8519141d19cbecd91bec864ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961a4d8519141d19cbecd91bec864ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e5d83b6ec6447cadf6f304dfb61349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961a4d8519141d19cbecd91bec864ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e5d83b6ec6447cadf6f304dfb61349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e551865d63462e94d25f7f47b9d0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961a4d8519141d19cbecd91bec864ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e5d83b6ec6447cadf6f304dfb61349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e551865d63462e94d25f7f47b9d0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(489864, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process dataset\n",
    "def preprocess_example(example: Dict[str, Any]):\n",
    "    numbers: List[int] = example[\"nums\"]\n",
    "    target: int = example[\"target\"]\n",
    "\n",
    "    prefix = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(numbers=numbers, target=target)},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prefix, tokenize=True, continue_final_message=True\n",
    "    )\n",
    "    prompt = tokenizer.decode(\n",
    "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"input_ids\": input_ids}\n",
    "\n",
    "# Note that the base model and \"instruct\" model have different eos token. \n",
    "# Here we make sure to use the correct one.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHAT_NAME)\n",
    "EOS_TOKEN_ID = AutoTokenizer.from_pretrained(MODEL_NAME).eos_token_id\n",
    "EOS_TOKEN = tokenizer.convert_ids_to_tokens(EOS_TOKEN_ID)\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "dataset = dataset.map(preprocess_example, num_proc=6)\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = dataset.train_test_split(test_size=500, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completion: str) -> float:\n",
    "    \"\"\"Check if output follows <think>...</think>\\n<answer>...</answer> format.\"\"\"\n",
    "    allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "\n",
    "    try:\n",
    "        completion = \"<think>\" + completion\n",
    "        if completion.endswith(EOS_TOKEN):\n",
    "            completion = completion[:-len(EOS_TOKEN)]\n",
    "\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            return 0.0\n",
    "        else:\n",
    "            answer_content = match.group(2).strip()\n",
    "            if not re.match(allowed_pattern, answer_content):\n",
    "                return 0.5\n",
    "            else:\n",
    "                return 1.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def equation_reward_func(completion: str, nums: List[int], target: int) -> float:\n",
    "    \"\"\"Check if the equation in <answer> tags evaluates to the target using all numbers exactly once.\"\"\"\n",
    "    try:\n",
    "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "        if match is None:\n",
    "            return 0.0\n",
    "\n",
    "        equation = match.group(1).strip()\n",
    "        used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n",
    "\n",
    "        if sorted(used_numbers) != sorted(nums):\n",
    "            return 0.0\n",
    "\n",
    "        allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "        if not re.match(allowed_pattern, equation):\n",
    "            return 0.0\n",
    "\n",
    "        result = eval(equation, {\"__builtins__\": None}, {})\n",
    "        if abs(float(result) - float(target)) < 1e-5:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "def compute_reward(completion: str, sample: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:\n",
    "    nums = sample[\"nums\"]\n",
    "    target = sample[\"target\"]\n",
    "\n",
    "    format_reward = format_reward_func(completion)\n",
    "    equation_reward = equation_reward_func(completion=completion, nums=nums, target=target)\n",
    "\n",
    "    reward = format_reward + equation_reward\n",
    "    metrics = {\n",
    "        \"format_reward\": format_reward,\n",
    "        \"equation_reward\": equation_reward,\n",
    "    }   \n",
    "\n",
    "    return reward, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_episodes(\n",
    "    samples: List[Dict[str, Any]],\n",
    "    all_generations: List[List[int]],\n",
    "    all_finish_reasons: List[str],\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"Process generations into training episodes with GRPO advantages.\"\"\"\n",
    "    assert len(all_generations) == len(all_finish_reasons)\n",
    "    assert len(all_generations) == len(samples) * GENERATIONS_PER_SAMPLE\n",
    "\n",
    "    groups = [\n",
    "        list(range(i, i + GENERATIONS_PER_SAMPLE))\n",
    "        for i in range(0, len(all_generations), GENERATIONS_PER_SAMPLE)\n",
    "    ]\n",
    "\n",
    "    all_query_token_ids, all_responses_token_ids, all_advantages = [], [], []\n",
    "\n",
    "    stats = {\n",
    "        \"response_lengths\": [],\n",
    "        \"rewards\": [],\n",
    "        \"non_stop_rate\": [],\n",
    "    }\n",
    "\n",
    "    for sample, group_indices in zip(samples, groups):\n",
    "        finish_reasons = [all_finish_reasons[i] for i in group_indices]\n",
    "        response_token_ids = [all_generations[i] for i in group_indices]\n",
    "        responses = tokenizer.batch_decode(response_token_ids, skip_special_tokens=False)\n",
    "\n",
    "        rewards_and_metrics = [compute_reward(resp, sample) for resp in responses]\n",
    "        rewards, reward_metrics = zip(*rewards_and_metrics)\n",
    "\n",
    "        # GRPO advantage: normalize rewards within the group\n",
    "        rewards = np.array(rewards)\n",
    "        response_advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4)\n",
    "        \n",
    "        # Assign same advantage to all tokens in each response\n",
    "        advantages = [\n",
    "            [resp_adv] * len(resp) \n",
    "            for resp_adv, resp in zip(response_advantages, response_token_ids)\n",
    "        ]\n",
    "\n",
    "        all_query_token_ids.extend([sample[\"input_ids\"]] * GENERATIONS_PER_SAMPLE)\n",
    "        all_responses_token_ids.extend(response_token_ids)\n",
    "        all_advantages.extend(advantages)\n",
    "\n",
    "        stats[\"rewards\"].extend(rewards)\n",
    "        stats[\"non_stop_rate\"].extend([fr != \"stop\" for fr in finish_reasons])\n",
    "        stats[\"response_lengths\"].extend([len(ids) for ids in response_token_ids])\n",
    "        for rm in reward_metrics:\n",
    "            for k, v in rm.items():\n",
    "                stats.setdefault(f\"reward_metrics/{k}\", []).append(v)\n",
    "\n",
    "    episodes = {\n",
    "        \"all_query_token_ids\": all_query_token_ids,\n",
    "        \"all_response_token_ids\": all_responses_token_ids,\n",
    "        \"all_advantages\": all_advantages,\n",
    "    }\n",
    "\n",
    "    return episodes, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pg_loss(\n",
    "    policy_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
    "    reference_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
    "    batch: Dict[str, torch.Tensor],\n",
    "    total_response_len: int,\n",
    ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    \"\"\"Compute policy gradient loss with KL penalty between policy and reference models.\"\"\"\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    advantages = batch[\"advantages\"]\n",
    "\n",
    "    model_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    labels_mask = (labels[..., 1:] != -100).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_logps = compute_token_log_probs(reference_model, model_inputs, TEMPERATURE)\n",
    "\n",
    "    logps = compute_token_log_probs(policy_model, model_inputs, TEMPERATURE)\n",
    "\n",
    "    # KL penalty (k3 estimator)\n",
    "    kl_penalty = torch.exp(ref_logps - logps) - (ref_logps - logps) - 1\n",
    "    kl_penalty = kl_penalty * labels_mask\n",
    "\n",
    "    entropy = -logps.sum() / labels_mask.sum()\n",
    "\n",
    "    # Policy gradient loss: -log_prob * advantage\n",
    "    policy_loss = -logps * advantages[..., 1:]\n",
    "    policy_loss = policy_loss * labels_mask\n",
    "\n",
    "    loss = (policy_loss + KL_COEFFICIENT * kl_penalty).sum() / total_response_len\n",
    "\n",
    "    metrics = {\n",
    "        \"policy_loss\": policy_loss.sum().item() / total_response_len,\n",
    "        \"kl_penalty\": kl_penalty.sum().item() / total_response_len,\n",
    "        \"entropy\": entropy.item() / total_response_len,\n",
    "    }\n",
    "\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761415cacaec488c975431886a1ffd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761415cacaec488c975431886a1ffd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1436ced14e45eebfdac98eeff5542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761415cacaec488c975431886a1ffd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1436ced14e45eebfdac98eeff5542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3de56e245942a09bee82b615777f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize main and reference models\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=0,\n",
    ")\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=0,\n",
    ")\n",
    "policy_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "\n",
    "# Initialize DeepSpeed engines\n",
    "policy_model, *_ = deepspeed.initialize(\n",
    "    model=policy_model,\n",
    "    config=deepspeed_config,\n",
    "    model_parameters=policy_model.parameters(),\n",
    ")\n",
    "reference_model, *_ = deepspeed.initialize(\n",
    "    model=reference_model,\n",
    "    config=ref_deepspeed_config,\n",
    ")\n",
    "\n",
    "reference_model.module.cpu()\n",
    "\n",
    "############################################\n",
    "# Initialize vLLM (Inference) engine\n",
    "############################################\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    skip_tokenizer_init=False,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    enable_prefix_caching=True,\n",
    "    swap_space=1,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2048,\n",
    "    enable_sleep_mode=True,\n",
    ")\n",
    "\n",
    "# Wandb for logging\n",
    "wandb.init(\n",
    "    project=\"r1-aha-moment\",\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_iterations\": NUM_ITERATIONS,\n",
    "        \"episodes_per_iteration\": EPISODES_PER_ITERATION,\n",
    "        \"rollouts_per_episode\": GENERATIONS_PER_SAMPLE,\n",
    "        \"kl_coefficient\": KL_COEFFICIENT,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "begin_iter = 0\n",
    "ckpt_path, ckpt_iter = find_last_checkpoint(EXP_DIR)\n",
    "if ckpt_path is not None:\n",
    "    print(f\"Resuming from checkpoint {ckpt_path} at iteration {ckpt_iter}\")\n",
    "    out = policy_model.load_checkpoint(ckpt_path / \"deepspeed\")\n",
    "    if out is None:\n",
    "        raise RuntimeError(f\"Failed to load checkpoint {ckpt_path}\")\n",
    "    begin_iter = ckpt_iter + 1\n",
    "    load_model_into_vllm(policy_model, inference_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in trange(NUM_ITERATIONS):\n",
    "    print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    #########################################################\n",
    "    # Evaluation\n",
    "    #########################################################\n",
    "\n",
    "    eval_stats = None\n",
    "    if iteration % 25 == 0:\n",
    "        print(\"Evaluating on eval set...\")\n",
    "        eval_episodes, eval_stats = evaluate_on_test_set(\n",
    "            inference_engine=inference_engine,\n",
    "            test_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            eos_token=EOS_TOKEN,\n",
    "            eval_sampling_params=SamplingParams(\n",
    "                temperature=0.3,\n",
    "                max_tokens=1024,\n",
    "                n=1,\n",
    "                detokenize=False,\n",
    "                stop_token_ids=[EOS_TOKEN_ID],\n",
    "            ),\n",
    "            reward_func=lambda completion, sample: compute_reward(\n",
    "                completion, sample\n",
    "            ),\n",
    "        )\n",
    "        eval_episode_table = dump_episodes(\n",
    "            episodes=eval_episodes,\n",
    "            episodes_stats=eval_stats,\n",
    "            exp_dir=EXP_DIR,\n",
    "            tokenizer=tokenizer,\n",
    "            iteration=iteration,\n",
    "            is_eval=True,\n",
    "        )\n",
    "        wandb.log({\"eval/episodes\": eval_episode_table, \"iteration\": iteration})\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Generate Episodes\n",
    "    #########################################################\n",
    "\n",
    "    # Sample training batch\n",
    "    num_samples = EPISODES_PER_ITERATION // GENERATIONS_PER_SAMPLE\n",
    "    indices = np.random.choice(\n",
    "        len(train_dataset), size=num_samples, replace=False\n",
    "    )\n",
    "    samples = train_dataset.select(indices)\n",
    "\n",
    "    # Sample responses\n",
    "    outputs = inference_engine.generate(\n",
    "        prompt_token_ids=samples[\"input_ids\"],\n",
    "        sampling_params=SamplingParams(\n",
    "            n=GENERATIONS_PER_SAMPLE,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            max_tokens=MAX_RESPONSE_TOKENS,\n",
    "            detokenize=False,\n",
    "            stop_token_ids=[EOS_TOKEN_ID],\n",
    "        )\n",
    "    )\n",
    "    all_generations = [list(g.token_ids) for out in outputs for g in out.outputs]\n",
    "    all_finish_reasons = [g.finish_reason for out in outputs for g in out.outputs]\n",
    "    inference_engine.sleep(1)\n",
    "\n",
    "    print(f\"Generated {len(all_generations)} responses\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Process responses and calculate rewards\n",
    "    episodes, episodes_stats = create_training_episodes(\n",
    "        samples,\n",
    "        all_generations,\n",
    "        all_finish_reasons,\n",
    "    )\n",
    "    for k, v in episodes_stats.items():\n",
    "        metrics.setdefault(k, []).extend(v)\n",
    "\n",
    "    episode_table = dump_episodes(\n",
    "        episodes=episodes,\n",
    "        episodes_stats=episodes_stats,\n",
    "        exp_dir=EXP_DIR,\n",
    "        tokenizer=tokenizer,\n",
    "        iteration=iteration,\n",
    "    )\n",
    "\n",
    "    #########################################################\n",
    "    # Training\n",
    "    #########################################################\n",
    "\n",
    "    # Prepare training batch\n",
    "    model_inputs = prepare_model_inputs(\n",
    "        query_token_ids=episodes[\"all_query_token_ids\"],\n",
    "        response_token_ids=episodes[\"all_response_token_ids\"],\n",
    "        advantages=episodes[\"all_advantages\"],\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    # Calculate losses and update model\n",
    "    policy_model.train()\n",
    "    reference_model.module.cuda()\n",
    "    reference_model.eval()\n",
    "\n",
    "    total_response_len = (model_inputs[\"labels\"] != -100).sum().item()\n",
    "\n",
    "    for i in trange(0, EPISODES_PER_ITERATION, PER_DEVICE_BATCH_SIZE, desc=\"Gradient Accumulation\"):\n",
    "        batch = {\n",
    "            k: v[i : i + PER_DEVICE_BATCH_SIZE]\n",
    "            for k, v in model_inputs.items()\n",
    "        }\n",
    "\n",
    "        # Compute policy gradient loss\n",
    "        loss, loss_metrics = compute_pg_loss(\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            batch=batch,\n",
    "            total_response_len=total_response_len,\n",
    "        )\n",
    "\n",
    "        # Track metrics\n",
    "        metrics.setdefault(\"loss\", []).append(loss.item())\n",
    "        grad_norm = policy_model.get_global_grad_norm()\n",
    "        if grad_norm is not None:\n",
    "            grad_norm = grad_norm.item()\n",
    "        metrics.setdefault(\"grad_norm\", []).append(grad_norm)\n",
    "        for k, v in loss_metrics.items():\n",
    "            metrics.setdefault(k, []).append(v.item() if isinstance(v, torch.Tensor) else v)\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        policy_model.backward(loss, scale_wrt_gas=False)\n",
    "        \n",
    "        # Free memory\n",
    "        del loss, loss_metrics\n",
    "        if policy_model.is_gradient_accumulation_boundary():\n",
    "            reference_model.module.cpu()\n",
    "\n",
    "        policy_model.step()\n",
    "\n",
    "    #########################################################\n",
    "    # Update inference engine weights\n",
    "    #########################################################\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "    inference_engine.wake_up()\n",
    "    load_model_into_vllm(policy_model, inference_engine)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Log metrics\n",
    "    #########################################################\n",
    "\n",
    "    train_metrics = {\n",
    "        k: np.mean(v) for k, v in metrics.items() if None not in v\n",
    "    }\n",
    "    train_metrics[\"learning_rate\"] = policy_model.get_lr()[0]\n",
    "    logs = {\n",
    "        \"iteration\": iteration,\n",
    "        f\"episodes/iter_{iteration:06d}\": episode_table,\n",
    "        **{f\"train/{k}\": v for k, v in train_metrics.items()},\n",
    "    }\n",
    "    if eval_stats is not None:\n",
    "        eval_metrics = {k: np.mean(v) for k, v in eval_stats.items() if None not in v}\n",
    "        logs.update({f\"eval/{k}\": v for k, v in eval_metrics.items()})\n",
    "    wandb.log(logs)\n",
    "\n",
    "    selected_keys = [\n",
    "        \"train/kl_penalty\",\n",
    "        \"train/rewards\",\n",
    "        \"train/reward_metrics/format_reward\",\n",
    "        \"train/reward_metrics/equation_reward\",\n",
    "        \"eval/rewards\",\n",
    "        \"eval/reward_metrics/format_reward\",\n",
    "        \"eval/reward_metrics/equation_reward\",\n",
    "    ]\n",
    "    selected_metrics = {k: logs[k] for k in selected_keys if k in logs}\n",
    "    print(f\"KEY METRICS: {selected_metrics}\")\n",
    "\n",
    "    if iteration % 50 == 0 and iteration != 0:\n",
    "        policy_model.module.save_pretrained(\n",
    "            str(EXP_DIR / \"checkpoints\" / f\"ckpt_{iteration:06d}\" / \"hf_model\")\n",
    "        )\n",
    "        policy_model.save_checkpoint(\n",
    "            str(EXP_DIR / \"checkpoints\" / f\"ckpt_{iteration:06d}\" / \"deepspeed\")\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

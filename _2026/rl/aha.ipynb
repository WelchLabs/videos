{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nanoAhaMoment: Single File R1-Zero Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Requirements\n",
    "\n",
    "Install the required packages:\n",
    "\n",
    "```txt\n",
    "# Core dependencies\n",
    "torch==2.9.1\n",
    "torchvision==0.24.1\n",
    "torchaudio==2.9.1\n",
    "\n",
    "# Deep learning frameworks\n",
    "transformers>=4.57.0\n",
    "datasets>=4.5.0\n",
    "deepspeed>=0.18.0\n",
    "accelerate\n",
    "\n",
    "# vLLM for inference\n",
    "vllm>=0.14.1\n",
    "\n",
    "# Training and logging\n",
    "wandb\n",
    "tqdm\n",
    "\n",
    "# Standard scientific computing\n",
    "numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(72110) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.9.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision==0.24.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (0.24.1)\n",
      "Requirement already satisfied: torchaudio==2.9.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch==2.9.1) (3.24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch==2.9.1) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch==2.9.1) (77.0.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch==2.9.1) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch==2.9.1) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch==2.9.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch==2.9.1) (2025.10.0)\n",
      "Requirement already satisfied: numpy in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torchvision==0.24.1) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torchvision==0.24.1) (12.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.9.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jinja2->torch==2.9.1) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(72114) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (4.57.6)\n",
      "Requirement already satisfied: datasets in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: deepspeed in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (0.18.6)\n",
      "Requirement already satisfied: accelerate in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: vllm in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (0.14.1)\n",
      "Requirement already satisfied: wandb in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (0.25.0)\n",
      "Requirement already satisfied: tqdm in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (4.67.3)\n",
      "Requirement already satisfied: numpy in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (2.2.6)\n",
      "Requirement already satisfied: filelock in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from transformers) (3.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from transformers) (0.36.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: einops in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from deepspeed) (0.8.2)\n",
      "Requirement already satisfied: hjson in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from deepspeed) (1.1.2)\n",
      "Requirement already satisfied: ninja in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from deepspeed) (1.13.0)\n",
      "Requirement already satisfied: psutil in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from deepspeed) (7.2.2)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from deepspeed) (2.12.5)\n",
      "Requirement already satisfied: torch in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from deepspeed) (2.9.1)\n",
      "Requirement already satisfied: cachetools in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (7.0.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.2.1)\n",
      "Requirement already satisfied: blake3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=6.30.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (6.33.5)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.129.0)\n",
      "Requirement already satisfied: openai>=1.99.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (2.21.0)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.24.1)\n",
      "Requirement already satisfied: pillow in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (12.1.1)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.12.0)\n",
      "Requirement already satisfied: lm-format-enforcer==0.11.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.11.3)\n",
      "Requirement already satisfied: llguidance<1.4.0,>=1.3.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.3.0)\n",
      "Requirement already satisfied: outlines_core==0.2.11 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.2.11)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (5.6.3)\n",
      "Requirement already satisfied: lark==1.2.2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.29 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.1.29)\n",
      "Requirement already satisfied: partial-json-parser in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.2.1.1.post7)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (27.1.0)\n",
      "Requirement already satisfied: msgspec in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: gguf>=0.17.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.17.1)\n",
      "Requirement already satisfied: mistral_common>=1.8.8 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mistral_common[image]>=1.8.8->vllm) (1.9.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.13.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (4.13.0.92)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.17.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=77.0.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (77.0.3)\n",
      "Requirement already satisfied: compressed-tensors==0.13.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.13.0)\n",
      "Requirement already satisfied: depyf==0.20.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (3.1.2)\n",
      "Requirement already satisfied: watchfiles in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.1.1)\n",
      "Requirement already satisfied: python-json-logger in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (4.0.0)\n",
      "Requirement already satisfied: pybase64 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.4.3)\n",
      "Requirement already satisfied: cbor2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (5.8.0)\n",
      "Requirement already satisfied: ijson in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (3.4.0.post0)\n",
      "Requirement already satisfied: setproctitle in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.3.7)\n",
      "Requirement already satisfied: openai-harmony>=0.0.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.0.8)\n",
      "Requirement already satisfied: anthropic==0.71.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.71.0)\n",
      "Requirement already satisfied: model-hosting-container-standards<1.0.0,>=0.1.10 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.1.13)\n",
      "Requirement already satisfied: mcp in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.26.0)\n",
      "Requirement already satisfied: grpcio>=1.76.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.78.0)\n",
      "Requirement already satisfied: grpcio-reflection>=1.76.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (1.78.0)\n",
      "Requirement already satisfied: numba==0.61.2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.61.2)\n",
      "Requirement already satisfied: torchaudio in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (2.9.1)\n",
      "Requirement already satisfied: torchvision in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from vllm) (0.24.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from anthropic==0.71.0->vllm) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from anthropic==0.71.0->vllm) (0.17.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from anthropic==0.71.0->vllm) (0.13.0)\n",
      "Requirement already satisfied: sniffio in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from anthropic==0.71.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: loguru in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from compressed-tensors==0.13.0->vllm) (0.7.3)\n",
      "Requirement already satisfied: astor in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from depyf==0.20.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from lm-format-enforcer==0.11.3->vllm) (0.3.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch->deepspeed) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch->deepspeed) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from torch->deepspeed) (3.1.6)\n",
      "Requirement already satisfied: mlx-lm in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from xgrammar==0.1.29->vllm) (0.29.1)\n",
      "Requirement already satisfied: jmespath in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.10->vllm) (1.1.0)\n",
      "Requirement already satisfied: starlette>=0.49.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.10->vllm) (0.52.1)\n",
      "Requirement already satisfied: supervisor>=4.2.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.10->vllm) (4.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from wandb) (3.1.46)\n",
      "Requirement already satisfied: platformdirs in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from wandb) (4.9.1)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from wandb) (2.53.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.0.4)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.21)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.22)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.40.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.13.0)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.11.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.23.1)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.19.4)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.12.0)\n",
      "Requirement already satisfied: rignore>=0.5.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.6)\n",
      "Requirement already satisfied: fastar>=0.8.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.8.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jinja2->torch->deepspeed) (3.0.3)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (4.26.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (0.30.0)\n",
      "Requirement already satisfied: pycountry>=23 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (24.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pydantic-settings>=2.0.0->fastapi[standard]>=0.115.0->vllm) (1.2.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.22.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (16.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mcp->vllm) (0.4.3)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (2.11.0)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mcp->vllm) (3.2.0)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (46.0.5)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (3.0)\n",
      "Requirement already satisfied: mlx>=0.29.2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mlx-lm->xgrammar==0.1.29->vllm) (0.30.6)\n",
      "Requirement already satisfied: mlx-metal==0.30.6 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from mlx>=0.29.2->mlx-lm->xgrammar==0.1.29->vllm) (0.30.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pranav/.pyenv/versions/3.12.12/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.9.1 torchvision==0.24.1 torchaudio==2.9.1\n",
    "!pip install transformers datasets deepspeed accelerate vllm wandb tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SCRATCH = Path.home() / \"scratch\"\n",
    "os.environ[\"HF_HOME\"] = str(SCRATCH / \"hf_home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0216 10:32:14.755000 63473 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0216 10:32:14.755000 63473 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-16 10:32:18 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "from typing import Any, Dict, List, Tuple, Union, TYPE_CHECKING\n",
    "\n",
    "# Workaround for torchvision circular import issue\n",
    "# Import torch and torchvision first to ensure proper initialization\n",
    "import torch\n",
    "import torchvision  # This needs to be imported early to avoid circular import issues\n",
    "\n",
    "import deepspeed\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from deepspeed import DeepSpeedEngine\n",
    "from tqdm import trange\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Direct imports to avoid vLLM lazy import issues\n",
    "from vllm.entrypoints.llm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from transformers import PreTrainedModel\n",
    "\n",
    "import wandb\n",
    "from utils import (\n",
    "    compute_token_log_probs,\n",
    "    dump_episodes,\n",
    "    evaluate_on_test_set,\n",
    "    find_free_port,\n",
    "    find_last_checkpoint,\n",
    "    prepare_model_inputs,\n",
    "    load_model_into_vllm\n",
    ")\n",
    "\n",
    "# Needed to stop DeepSpeed from complaining\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs and Checkpoints will be saved to: /Users/pranav/scratch/deepseek_r1z_hackathon/r1-zero\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "MODEL_CHAT_NAME = MODEL_NAME + \"-Instruct\"\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "\n",
    "# Total number of training iterations\n",
    "NUM_ITERATIONS = 1000\n",
    "# Number of episodes to collect per iteration for training\n",
    "EPISODES_PER_ITERATION = 64\n",
    "# Number of responses to generate for each input prompt (i.e. group size in GRPO)\n",
    "GENERATIONS_PER_SAMPLE = 4\n",
    "# Controls how much the policy can deviate from the reference model\n",
    "KL_COEFFICIENT = 0.001\n",
    "\n",
    "# Training hyperparameters\n",
    "# Batch size for each GPU device during training\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "# Learning rate for model updates\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "# Sampling parameters\n",
    "# Maximum number of tokens to generate in each response\n",
    "MAX_RESPONSE_TOKENS = 1024\n",
    "# Controls randomness in generation (higher = more random)\n",
    "TEMPERATURE = 1.0\n",
    "# Nucleus sampling parameter (1.0 = disabled)\n",
    "TOP_P = 1.0\n",
    "# Top-k sampling parameter (-1 = disabled)\n",
    "TOP_K = -1  # no top k\n",
    "\n",
    "# DeepSpeed configuration\n",
    "# DeepSpeed config for the policy model\n",
    "deepspeed_config = {\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\"stage\": 2, \"overlap_comm\": False},\n",
    "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
    "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": LEARNING_RATE,\n",
    "            \"betas\": (0.9, 0.999),\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"torch_adam\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "# DeepSpeed config for the reference model\n",
    "ref_deepspeed_config = {\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    # Note that we don't train the reference model\n",
    "    # These are just for compatibility with DeepSpeed.\n",
    "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
    "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
    "}\n",
    "\n",
    "RUN_NAME = \"r1-zero\"\n",
    "EXP_DIR = SCRATCH / \"deepseek_r1z_hackathon\" / RUN_NAME\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Logs and Checkpoints will be saved to: {EXP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the training prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = (\n",
    "    \"You are a helpful assistant. You first think about the reasoning process in the mind \"\n",
    "    \"and then provide the user with the answer.\"\n",
    ")\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Using the numbers {numbers}, create an equation that equals {target}. \"\n",
    "    \"You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. \"\n",
    "    \"Show your work in <think> </think> tags. And return the final equation and answer in \"\n",
    "    \"<answer> </answer> tags, for example <answer>(1 + 2) / (3 * 5)</answer>.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961a4d8519141d19cbecd91bec864ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961a4d8519141d19cbecd91bec864ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e5d83b6ec6447cadf6f304dfb61349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961a4d8519141d19cbecd91bec864ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e5d83b6ec6447cadf6f304dfb61349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e551865d63462e94d25f7f47b9d0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b3319364249cea59c77ff370307f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1f0cb079442cca93a384242e1fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a87cd478a4e4aa188acaf91aefe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a659dd07494dc38190fa00135b227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1738fec644798addd4d665070d1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ae5af24ba463f8b0a333700debeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f225737924e029c1f99cb133f5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ebd50b4e2f4a9e925f24d3ea0f35f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba61f347a25d4fa5a09f557b60b1adf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4961a4d8519141d19cbecd91bec864ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e5d83b6ec6447cadf6f304dfb61349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e551865d63462e94d25f7f47b9d0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/490364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(489864, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process dataset\n",
    "def preprocess_example(example: Dict[str, Any]):\n",
    "    numbers: List[int] = example[\"nums\"]\n",
    "    target: int = example[\"target\"]\n",
    "\n",
    "    prefix = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(numbers=numbers, target=target)},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prefix, tokenize=True, continue_final_message=True\n",
    "    )\n",
    "    prompt = tokenizer.decode(\n",
    "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"input_ids\": input_ids}\n",
    "\n",
    "# Note that the base model and \"instruct\" model have different eos token. \n",
    "# Here we make sure to use the correct one.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHAT_NAME)\n",
    "EOS_TOKEN_ID = AutoTokenizer.from_pretrained(MODEL_NAME).eos_token_id\n",
    "EOS_TOKEN = tokenizer.convert_ids_to_tokens(EOS_TOKEN_ID)\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "dataset = dataset.map(preprocess_example, num_proc=6)\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = dataset.train_test_split(test_size=500, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completion: str) -> float:\n",
    "    \"\"\"Check if output follows <think>...</think>\\n<answer>...</answer> format.\"\"\"\n",
    "    allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "\n",
    "    try:\n",
    "        completion = \"<think>\" + completion\n",
    "        if completion.endswith(EOS_TOKEN):\n",
    "            completion = completion[:-len(EOS_TOKEN)]\n",
    "\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            return 0.0\n",
    "        else:\n",
    "            answer_content = match.group(2).strip()\n",
    "            if not re.match(allowed_pattern, answer_content):\n",
    "                return 0.5\n",
    "            else:\n",
    "                return 1.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def equation_reward_func(completion: str, nums: List[int], target: int) -> float:\n",
    "    \"\"\"Check if the equation in <answer> tags evaluates to the target using all numbers exactly once.\"\"\"\n",
    "    try:\n",
    "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "        if match is None:\n",
    "            return 0.0\n",
    "\n",
    "        equation = match.group(1).strip()\n",
    "        used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n",
    "\n",
    "        if sorted(used_numbers) != sorted(nums):\n",
    "            return 0.0\n",
    "\n",
    "        allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "        if not re.match(allowed_pattern, equation):\n",
    "            return 0.0\n",
    "\n",
    "        result = eval(equation, {\"__builtins__\": None}, {})\n",
    "        if abs(float(result) - float(target)) < 1e-5:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "def compute_reward(completion: str, sample: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:\n",
    "    nums = sample[\"nums\"]\n",
    "    target = sample[\"target\"]\n",
    "\n",
    "    format_reward = format_reward_func(completion)\n",
    "    equation_reward = equation_reward_func(completion=completion, nums=nums, target=target)\n",
    "\n",
    "    reward = format_reward + equation_reward\n",
    "    metrics = {\n",
    "        \"format_reward\": format_reward,\n",
    "        \"equation_reward\": equation_reward,\n",
    "    }   \n",
    "\n",
    "    return reward, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_episodes(\n",
    "    samples: List[Dict[str, Any]],\n",
    "    all_generations: List[List[int]],\n",
    "    all_finish_reasons: List[str],\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"Process generations into training episodes with GRPO advantages.\"\"\"\n",
    "    assert len(all_generations) == len(all_finish_reasons)\n",
    "    assert len(all_generations) == len(samples) * GENERATIONS_PER_SAMPLE\n",
    "\n",
    "    groups = [\n",
    "        list(range(i, i + GENERATIONS_PER_SAMPLE))\n",
    "        for i in range(0, len(all_generations), GENERATIONS_PER_SAMPLE)\n",
    "    ]\n",
    "\n",
    "    all_query_token_ids, all_responses_token_ids, all_advantages = [], [], []\n",
    "\n",
    "    stats = {\n",
    "        \"response_lengths\": [],\n",
    "        \"rewards\": [],\n",
    "        \"non_stop_rate\": [],\n",
    "    }\n",
    "\n",
    "    for sample, group_indices in zip(samples, groups):\n",
    "        finish_reasons = [all_finish_reasons[i] for i in group_indices]\n",
    "        response_token_ids = [all_generations[i] for i in group_indices]\n",
    "        responses = tokenizer.batch_decode(response_token_ids, skip_special_tokens=False)\n",
    "\n",
    "        rewards_and_metrics = [compute_reward(resp, sample) for resp in responses]\n",
    "        rewards, reward_metrics = zip(*rewards_and_metrics)\n",
    "\n",
    "        # GRPO advantage: normalize rewards within the group\n",
    "        rewards = np.array(rewards)\n",
    "        response_advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4)\n",
    "        \n",
    "        # Assign same advantage to all tokens in each response\n",
    "        advantages = [\n",
    "            [resp_adv] * len(resp) \n",
    "            for resp_adv, resp in zip(response_advantages, response_token_ids)\n",
    "        ]\n",
    "\n",
    "        all_query_token_ids.extend([sample[\"input_ids\"]] * GENERATIONS_PER_SAMPLE)\n",
    "        all_responses_token_ids.extend(response_token_ids)\n",
    "        all_advantages.extend(advantages)\n",
    "\n",
    "        stats[\"rewards\"].extend(rewards)\n",
    "        stats[\"non_stop_rate\"].extend([fr != \"stop\" for fr in finish_reasons])\n",
    "        stats[\"response_lengths\"].extend([len(ids) for ids in response_token_ids])\n",
    "        for rm in reward_metrics:\n",
    "            for k, v in rm.items():\n",
    "                stats.setdefault(f\"reward_metrics/{k}\", []).append(v)\n",
    "\n",
    "    episodes = {\n",
    "        \"all_query_token_ids\": all_query_token_ids,\n",
    "        \"all_response_token_ids\": all_responses_token_ids,\n",
    "        \"all_advantages\": all_advantages,\n",
    "    }\n",
    "\n",
    "    return episodes, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pg_loss(\n",
    "    policy_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
    "    reference_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
    "    batch: Dict[str, torch.Tensor],\n",
    "    total_response_len: int,\n",
    ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    \"\"\"Compute policy gradient loss with KL penalty between policy and reference models.\"\"\"\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    advantages = batch[\"advantages\"]\n",
    "\n",
    "    model_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    labels_mask = (labels[..., 1:] != -100).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_logps = compute_token_log_probs(reference_model, model_inputs, TEMPERATURE)\n",
    "\n",
    "    logps = compute_token_log_probs(policy_model, model_inputs, TEMPERATURE)\n",
    "\n",
    "    # KL penalty (k3 estimator)\n",
    "    kl_penalty = torch.exp(ref_logps - logps) - (ref_logps - logps) - 1\n",
    "    kl_penalty = kl_penalty * labels_mask\n",
    "\n",
    "    entropy = -logps.sum() / labels_mask.sum()\n",
    "\n",
    "    # Policy gradient loss: -log_prob * advantage\n",
    "    policy_loss = -logps * advantages[..., 1:]\n",
    "    policy_loss = policy_loss * labels_mask\n",
    "\n",
    "    loss = (policy_loss + KL_COEFFICIENT * kl_penalty).sum() / total_response_len\n",
    "\n",
    "    metrics = {\n",
    "        \"policy_loss\": policy_loss.sum().item() / total_response_len,\n",
    "        \"kl_penalty\": kl_penalty.sum().item() / total_response_len,\n",
    "        \"entropy\": entropy.item() / total_response_len,\n",
    "    }\n",
    "\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761415cacaec488c975431886a1ffd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761415cacaec488c975431886a1ffd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1436ced14e45eebfdac98eeff5542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761415cacaec488c975431886a1ffd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1436ced14e45eebfdac98eeff5542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3de56e245942a09bee82b615777f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize main and reference models\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=0,\n",
    ")\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=0,\n",
    ")\n",
    "policy_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "\n",
    "# Initialize DeepSpeed engines\n",
    "policy_model, *_ = deepspeed.initialize(\n",
    "    model=policy_model,\n",
    "    config=deepspeed_config,\n",
    "    model_parameters=policy_model.parameters(),\n",
    ")\n",
    "reference_model, *_ = deepspeed.initialize(\n",
    "    model=reference_model,\n",
    "    config=ref_deepspeed_config,\n",
    ")\n",
    "\n",
    "reference_model.module.cpu()\n",
    "\n",
    "############################################\n",
    "# Initialize vLLM (Inference) engine\n",
    "############################################\n",
    "\n",
    "inference_engine = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    skip_tokenizer_init=False,\n",
    "    gpu_memory_utilization=0.2,\n",
    "    enable_prefix_caching=True,\n",
    "    swap_space=1,\n",
    "    scheduling_policy=\"fcfs\",\n",
    "    dtype=torch.bfloat16,\n",
    "    max_model_len=2048,\n",
    "    enable_sleep_mode=True,\n",
    ")\n",
    "\n",
    "# Wandb for logging\n",
    "wandb.init(\n",
    "    project=\"r1-aha-moment\",\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_iterations\": NUM_ITERATIONS,\n",
    "        \"episodes_per_iteration\": EPISODES_PER_ITERATION,\n",
    "        \"rollouts_per_episode\": GENERATIONS_PER_SAMPLE,\n",
    "        \"kl_coefficient\": KL_COEFFICIENT,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "begin_iter = 0\n",
    "ckpt_path, ckpt_iter = find_last_checkpoint(EXP_DIR)\n",
    "if ckpt_path is not None:\n",
    "    print(f\"Resuming from checkpoint {ckpt_path} at iteration {ckpt_iter}\")\n",
    "    out = policy_model.load_checkpoint(ckpt_path / \"deepspeed\")\n",
    "    if out is None:\n",
    "        raise RuntimeError(f\"Failed to load checkpoint {ckpt_path}\")\n",
    "    begin_iter = ckpt_iter + 1\n",
    "    load_model_into_vllm(policy_model, inference_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in trange(NUM_ITERATIONS):\n",
    "    print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    #########################################################\n",
    "    # Evaluation\n",
    "    #########################################################\n",
    "\n",
    "    eval_stats = None\n",
    "    if iteration % 25 == 0:\n",
    "        print(\"Evaluating on eval set...\")\n",
    "        eval_episodes, eval_stats = evaluate_on_test_set(\n",
    "            inference_engine=inference_engine,\n",
    "            test_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            eos_token=EOS_TOKEN,\n",
    "            eval_sampling_params=SamplingParams(\n",
    "                temperature=0.3,\n",
    "                max_tokens=1024,\n",
    "                n=1,\n",
    "                detokenize=False,\n",
    "                stop_token_ids=[EOS_TOKEN_ID],\n",
    "            ),\n",
    "            reward_func=lambda completion, sample: compute_reward(\n",
    "                completion, sample\n",
    "            ),\n",
    "        )\n",
    "        eval_episode_table = dump_episodes(\n",
    "            episodes=eval_episodes,\n",
    "            episodes_stats=eval_stats,\n",
    "            exp_dir=EXP_DIR,\n",
    "            tokenizer=tokenizer,\n",
    "            iteration=iteration,\n",
    "            is_eval=True,\n",
    "        )\n",
    "        wandb.log({\"eval/episodes\": eval_episode_table, \"iteration\": iteration})\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Generate Episodes\n",
    "    #########################################################\n",
    "\n",
    "    # Sample training batch\n",
    "    num_samples = EPISODES_PER_ITERATION // GENERATIONS_PER_SAMPLE\n",
    "    indices = np.random.choice(\n",
    "        len(train_dataset), size=num_samples, replace=False\n",
    "    )\n",
    "    samples = train_dataset.select(indices)\n",
    "\n",
    "    # Sample responses\n",
    "    outputs = inference_engine.generate(\n",
    "        prompt_token_ids=samples[\"input_ids\"],\n",
    "        sampling_params=SamplingParams(\n",
    "            n=GENERATIONS_PER_SAMPLE,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            max_tokens=MAX_RESPONSE_TOKENS,\n",
    "            detokenize=False,\n",
    "            stop_token_ids=[EOS_TOKEN_ID],\n",
    "        )\n",
    "    )\n",
    "    all_generations = [list(g.token_ids) for out in outputs for g in out.outputs]\n",
    "    all_finish_reasons = [g.finish_reason for out in outputs for g in out.outputs]\n",
    "    inference_engine.sleep(1)\n",
    "\n",
    "    print(f\"Generated {len(all_generations)} responses\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Process responses and calculate rewards\n",
    "    episodes, episodes_stats = create_training_episodes(\n",
    "        samples,\n",
    "        all_generations,\n",
    "        all_finish_reasons,\n",
    "    )\n",
    "    for k, v in episodes_stats.items():\n",
    "        metrics.setdefault(k, []).extend(v)\n",
    "\n",
    "    episode_table = dump_episodes(\n",
    "        episodes=episodes,\n",
    "        episodes_stats=episodes_stats,\n",
    "        exp_dir=EXP_DIR,\n",
    "        tokenizer=tokenizer,\n",
    "        iteration=iteration,\n",
    "    )\n",
    "\n",
    "    #########################################################\n",
    "    # Training\n",
    "    #########################################################\n",
    "\n",
    "    # Prepare training batch\n",
    "    model_inputs = prepare_model_inputs(\n",
    "        query_token_ids=episodes[\"all_query_token_ids\"],\n",
    "        response_token_ids=episodes[\"all_response_token_ids\"],\n",
    "        advantages=episodes[\"all_advantages\"],\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    # Calculate losses and update model\n",
    "    policy_model.train()\n",
    "    reference_model.module.cuda()\n",
    "    reference_model.eval()\n",
    "\n",
    "    total_response_len = (model_inputs[\"labels\"] != -100).sum().item()\n",
    "\n",
    "    for i in trange(0, EPISODES_PER_ITERATION, PER_DEVICE_BATCH_SIZE, desc=\"Gradient Accumulation\"):\n",
    "        batch = {\n",
    "            k: v[i : i + PER_DEVICE_BATCH_SIZE]\n",
    "            for k, v in model_inputs.items()\n",
    "        }\n",
    "\n",
    "        # Compute policy gradient loss\n",
    "        loss, loss_metrics = compute_pg_loss(\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            batch=batch,\n",
    "            total_response_len=total_response_len,\n",
    "        )\n",
    "\n",
    "        # Track metrics\n",
    "        metrics.setdefault(\"loss\", []).append(loss.item())\n",
    "        grad_norm = policy_model.get_global_grad_norm()\n",
    "        if grad_norm is not None:\n",
    "            grad_norm = grad_norm.item()\n",
    "        metrics.setdefault(\"grad_norm\", []).append(grad_norm)\n",
    "        for k, v in loss_metrics.items():\n",
    "            metrics.setdefault(k, []).append(v.item() if isinstance(v, torch.Tensor) else v)\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        policy_model.backward(loss, scale_wrt_gas=False)\n",
    "        \n",
    "        # Free memory\n",
    "        del loss, loss_metrics\n",
    "        if policy_model.is_gradient_accumulation_boundary():\n",
    "            reference_model.module.cpu()\n",
    "\n",
    "        policy_model.step()\n",
    "\n",
    "    #########################################################\n",
    "    # Update inference engine weights\n",
    "    #########################################################\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "    inference_engine.wake_up()\n",
    "    load_model_into_vllm(policy_model, inference_engine)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Log metrics\n",
    "    #########################################################\n",
    "\n",
    "    train_metrics = {\n",
    "        k: np.mean(v) for k, v in metrics.items() if None not in v\n",
    "    }\n",
    "    train_metrics[\"learning_rate\"] = policy_model.get_lr()[0]\n",
    "    logs = {\n",
    "        \"iteration\": iteration,\n",
    "        f\"episodes/iter_{iteration:06d}\": episode_table,\n",
    "        **{f\"train/{k}\": v for k, v in train_metrics.items()},\n",
    "    }\n",
    "    if eval_stats is not None:\n",
    "        eval_metrics = {k: np.mean(v) for k, v in eval_stats.items() if None not in v}\n",
    "        logs.update({f\"eval/{k}\": v for k, v in eval_metrics.items()})\n",
    "    wandb.log(logs)\n",
    "\n",
    "    selected_keys = [\n",
    "        \"train/kl_penalty\",\n",
    "        \"train/rewards\",\n",
    "        \"train/reward_metrics/format_reward\",\n",
    "        \"train/reward_metrics/equation_reward\",\n",
    "        \"eval/rewards\",\n",
    "        \"eval/reward_metrics/format_reward\",\n",
    "        \"eval/reward_metrics/equation_reward\",\n",
    "    ]\n",
    "    selected_metrics = {k: logs[k] for k in selected_keys if k in logs}\n",
    "    print(f\"KEY METRICS: {selected_metrics}\")\n",
    "\n",
    "    if iteration % 50 == 0 and iteration != 0:\n",
    "        policy_model.module.save_pretrained(\n",
    "            str(EXP_DIR / \"checkpoints\" / f\"ckpt_{iteration:06d}\" / \"hf_model\")\n",
    "        )\n",
    "        policy_model.save_checkpoint(\n",
    "            str(EXP_DIR / \"checkpoints\" / f\"ckpt_{iteration:06d}\" / \"deepspeed\")\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
